{"distilbart": "Lipid-Pro is a modular and user friendly, modular and client-based database management system for the identification of pre-defined lipid species (termed candidates) in large liquid mass spectrometry data acquired with LC-MS/MS using rt-aligned monoisotopic masses of molecular ions (MS) and fragment ion (MS). Since LipidBlast provides a module for construction of lipid species and their characteristic fragments using a flexible building block approach, we developed the Lipid-pro software solution for rapid identification of lipids in preprocessed DIA data. The application can be configured on Microsoft Windows platforms following a simple six-step installation process. We developed LipID-Pro as a desktop application (a desktop application), helpful for the Identification of polypeptide lipid species including name, rt, m/z as well as the peak areas of lipid molecules and their fragments. As output, Lipid Pro delivers information on the identified lipid species", "gold": "UNLABELLED A major challenge for mass spectrometric-based lipidomics, aiming at describing all lipid species in a biological sample, lies in the computational and bioinformatic processing of the large amount of data that arises after data acquisition. Lipid-Pro is a software tool that supports the identification of lipids by interpreting large datasets generated by liquid chromatography--tandem mass spectrometry (LC-MS/MS) using the advanced data-independent acquisition mode MS(E). In the MS(E) mode, the instrument fragments all molecular ions generated from a sample and records time-resolved molecular ion data as well as fragment ion data for every detectable molecular ion. Lipid-Pro matches the retention time-aligned mass-to-charge ratio data of molecular- and fragment ions with a lipid database and generates a report on all identified lipid species. For generation of the lipid database, Lipid-Pro provides a module for construction of lipid species and their fragments using a flexible building block approach. Hence, Lipid-Pro is an easy to use analysis tool to interpret complex MS(E) lipidomics data and also offers a module to generate a user-specific lipid database. AVAILABILITY AND IMPLEMENTATION Lipid-Pro is freely available at: http://www.neurogenetics.biozentrum.uni-wuerzburg.de/en/project/services/lipidpro/. SUPPLEMENTARY INFORMATION Supplementary data are available at Bioinformatics online.", "context": "We developed the Lipid-Pro software solution for rapid identification of lipids in preprocessed data-independent acquisition (DIA) data acquired with LC-MS/MS using rt-aligned monoisotopic masses of molecular ions (MS) and fragment ions. Since lipid species are identified by matching the measured MS/MS spectra to an in silico generated library after converting raw data into the MGF format, it is laborious to match hundreds of different lipid species of the database with a custom-based or online database. To meet the technological objectives of this research, we took a step forward in the development of a new user friendly, modular and client-based database management system. The application provides a module for construction of lipid species and their characteristic fragments using a flexible building block approach, rare or even non-described lipids beside the common species can be identified in total lipid extracts of biological samples. By comparison with the self-generated lipid database, pre-", "contribution": "We developed the Lipid-Pro software solution for rapid identification of lipids in preprocessed DIA data acquired with LC-MS/MS using rt-aligned monoisotopic masses of molecular ions (MS) and fragment ions (MGF). By comparison with the self-generated lipid database, pre-defined lipids are identified in the pre-processed datasets by matching the measured MS/MS spectra to an in silico generated library after converting raw data into the MGF format. The application provides a module for construction of lipid species and their characteristic fragments using a flexible building block approach, rare or even non-described lipids beside the common species can be identified in total lipid extracts of biological samples. To meet the technological objectives of this research, we took a step forward in the development of a new user friendly, modular and client based database management system.", "url": "https://doi.org/10.1093/bioinformatics/btu796", "title": "Lipid-Pro: a computational lipid identification solution for untargeted lipidomics on data-independent acquisition tandem mass spectrometry platforms."}
{"distilbart": "Despite intense efforts over decades, up to this date no one has succeeded in finding a finite bounded cake cutting protocol that guarantees envy-freeness for any number of agents. The main applications of cake cutting are fair scheduling, resource allocation and conflict resolution [Deng et al. 2012]. Since various important important divisible resources such as time and land can be captured by cake cutting, the problem of fairly dividing the cake is a fundamental one within the area of fair division and multiagent resource allocation [Brams and Taylor 1996; Robertson and Webb 1998; Stewart 1999;Stew 1999; Thomson 2007]. Although an envy-free allocation is guaranteed to exist even with n-1 cuts [Su1999] 1], finding an envy free allocation is a challenging problem which has been termed \"one of the most important open problems in 20th century mathematics\" by Garfunkel [1988]. Since the valuations of agents over the cake can be complex, eliciting each agent's complete valuations", "gold": "We consider the well-studied cake cutting problem in which the goal is to find an envy-free allocation based on queries from $n$ agents. The problem has received attention in computer science, mathematics, and economics. It has been a major open problem whether there exists a discrete and bounded envy-free protocol. We resolve the problem by proposing a discrete and bounded envy-free protocol for any number of agents. The maximum number of queries required by the protocol is $n^{n^{n^{n^{n^n}}}}$. We additionally show that even if we do not run our protocol to completion, it can find in at most $n^3{(n^2)}^n$ queries a partial allocation of the cake that achieves proportionality (each agent gets at least $1/n$ of the value of the whole cake) and envy-freeness. Finally we show that an envy-free partial allocation can be computed in at most $n^3{(n^2)}^n$ queries such that each agent gets a connected piece that gives the agent at least $1/(3n)$ of the value of the whole cake.", "context": "Despite intense efforts over decades, up to this date no one has succeeded in finding a finite bounded cake cutting protocol that guarantees envy-freeness for any number of agents. In this paper, we settle the problem by presenting a general discrete and bounded envy-free cake cutting protocols. The protocol is for the cake cutting setting that is a versatile mathematical model for allocation of a heterogeneous divisible good among multiple agents with possibly different preferences over different parts of the cake. The main applications of cake cutting are fair scheduling, resource allocation, and conflict resolution.", "contribution": "In this paper, we settle the problem by presenting a general discrete envy-free protocol for any number of agents. The protocol is for the cake cutting setting that is a versatile mathematical model for allocation of a heterogeneous divisible good among multiple agents with possibly different preferences over different parts of the cake.", "url": "https://doi.org/10.1109/FOCS.2016.52", "title": "A Discrete and Bounded Envy-Free Cake Cutting Protocol for Any Number of Agents"}
{"distilbart": "In this paper we propose a general line search scheme that is applicable to most first-order convex optimization methods, including those mentioned above whose convergence proofs are not based on the decrease of an observable quantity. We exploit the fact that many first order optimization algorithms can be viewed as averaged iterations of some nonexpansive operator, i.e., it satisfies Su\u2212Sv2 \u2264 u \u2212 v 2 for all u, v. The superscript k denotes iteration number. The middle expression shows that the next point is a weighted average of the current point and Sxk in the direction of the residual. The expression on the righthand side of (1) show that the iteration can be interpreted as a taking step of length\u1fb1 in the directions of the fixed-point residual. Assuming a fixed point exists, the iteration converges to the set of fixed-points. In the general case, then there is a trade-off between the cost of the line search", "gold": "Many popular first order algorithms for convex optimization, such as forward-backward splitting, Douglas-Rachford splitting, and the alternating direction method of multipliers (ADMM), can be formulated as averaged iteration of a nonexpansive mapping. In this paper we propose a line search for averaged iteration that preserves the theoretical convergence guarantee, while often accelerating practical convergence. We discuss several general cases in which the additional computational cost of the line search is modest compared to the savings obtained.", "context": "In this paper we propose a general line search scheme that is applicable to most first-order convex optimization methods, including those mentioned above whose convergence proofs are not based on the decrease of an observable quantity. We exploit the fact that many first order optimization algorithms can be viewed as averaged iterations of some nonexpansive operator, i.e., they can be written in the form $x_k+1 = x_k + \\alpha^k (Sx_{k - x k})$. The middle expression shows that the next point is a weighted average of the current point $X_k$ and $\\sigma_k>0$, where $\\mathcal{R}^{n}\\to\\mathbb R^n$ denotes iteration number. Assuming a fixed-point exists, the iteration converges to the set of fixed-points. In this case, then, there is a trade-off between the cost of the line search (which depends on the number", "contribution": "In this paper we propose a general line search scheme that is applicable to most first-order convex optimization methods, including those mentioned above whose convergence proofs are not based on the decrease of an observable quantity. We exploit the fact that many first order optimization algorithms can be viewed as averaged iterations of some nonexpansive operator, i.e., they can be written in the form $X_k+1 = (1 -\\epsilon) x_{k + \\epsilons} Sx_{k} + \\alpha^k (Sx_k-x_n)$. The middle expression shows that the next point is a weighted average of the current point and $\\sigma_k$ and $\\mathcal{S}(0, 1)$, where $S:R^{N}\\to R^{n}$ and $S :R^{n\\rightarrow \\mathbb{R}^{n})$ are nonnegative functions with respect", "url": "https://doi.org/10.1109/CDC.2016.7798401", "title": "Line Search for Averaged Operator Iteration"}
{"distilbart": "In order to support this study we have implemented a set of generic content-based filtering, collaborative filtering and social recommendation approaches for social systems. We compare the performance of the recommenders with ranking quality metrics from the Information Retrieval field, namely precision, recall and nDCG. Second, we compare additional characteristics that measure coverage, diversity, novelty, and overlap between items. By using these recommenders and datasets we conduct a twofold experiment. First, we compared the results of the recommendeders and non-performance metrics on the recommendation approaches studied in RQ1 (RQ1) and RQ2 (rQ3). The proposed recommendations are based on the use of different sources of information, including ratings, tags and social contacts. In addition, we show considerable room for progress in such, to a large extent, unexplored dimensions. The combination of sources and evaluation methodologies thus present themselves as interrelated research goals, where we see in the former direction for improvement", "gold": "AbstractWhile recommendation approaches exploiting different input sources have started to proliferate in the literature, an explicit study of the effect of the combination of heterogeneous inputs is still missing. On the other hand, in this context there are sides to recommendation quality requiring further characterisation and methodological research -a gap that is acknowledged in the field. We present a comparative study on the influence that different types of information available in social systems have on item recommendation. Aiming to identify which sources of user interest evidence -tags, social contacts, and user-item interaction data-are more effective to achieve useful recommendations, and in what aspect, we evaluate a number of content-based, collaborative filtering, and social recommenders on three datasets obtained from Delicious, Last.fm, and MovieLens. Aiming to determine whether and how combining such information sources may enhance over individual recommendation approaches, we extend the common accuracy-oriented evaluation practice with various metrics to measure further recommendation quality dimensions, namely coverage, diversity, novelty, overlap, and relative diversity between ranked item recommendations. We report empiric observations showing that exploiting tagging information by content-based recommenders provides high coverage and novelty, and combining social networking and collaborative filtering information by hybrid recommenders results in high accuracy and diversity. This, along with the fact that recommendation lists from the evaluated approaches had low overlap and relative diversity values between them, gives insights that meta-hybrid recommenders combining the above strategies may provide valuable, balanced item suggestions in terms of performance and non-performance metrics.", "context": "In recent years it has been made clear that a single algorithm is generally insufficient to optimise the effectiveness of recommendations. Likewise, recommendation based on a single source of input data is generally suboptimal, inasmuch as the multiplicity of available hints for good recommendations are missed. At the same time, the purpose and scenarios for recommendation are diverse, and consequently, a single view of recommendation quality becomes insufficient to assess the value and usefulness of a recommendation approach. Evaluation methodologies and metrics need to be extended for this purpose, which is currently an open area of research and development in the field [8-51]. In order to support this study we have implemented a set of generic content-based filtering, collaborative filtering, and social recommendation approaches for social systems, and have built three datasets with information of different types obtained from Delicious 2, Last.fm 3 and MovieLens 4. By using these recommenders and datasets we conduct a twofold experiment. First, we compare the performance of", "contribution": "In order to support this study we have implemented a set of generic content-based filtering, collaborative filtering and social recommendation approaches for social systems, and have built three datasets with information of different types obtained from Delicious 2, Last.fm 3 and MovieLens 4. By using these recommenders and datasets we conduct a twofold experiment. First, we compare the performance of the recommenders with ranking quality metrics from the Information Retrieval field, namely precision, recall and nDCG. Second, we compared additional characteristics of the recommendations with a number of novel metrics that measure coverage, diversity, novelty, and overlap of and between ranked lists of recommended items. The experimental setup of the study describes the utilised datasets, the proposed performance and non-performance metrics, and the followed evaluation protocol.", "url": "https://doi.org/10.1016/j.ins.2012.09.039", "title": "A comparative study of heterogeneous item recommendations in social systems"}
{"distilbart": "This paper describes an approach to addressing the problem of self-delusion and other problems with agent behavior. It is illustrated by two simple examples. The fourth section presents the model-based approach to defining utility functions for agents with preprogrammed environment models, but it should work for agents designed to act in the physical world. This approach may fail to match any structures in the agent's learned environment model, in which case the agent fails (or continues to search its environment for structures that do match the specifications). This requires that utility function definitions be based on some assumptions about the environment. For example an agent designer may know that the environment contains certain kinds of objects and define the utility function in terms of the states of those objects. The utility function may initially be defined in term of specifications that will match those objects and their states in the learning environment model. The specifications in an agent's utility function definition might fail to link any structures within the agent\u2019s learned environment Model, in", "gold": "Orseau and Ring, as well as Dewey, have recently described problems, including self-delusion, with the behavior of agents using various definitions of utility functions. An agent's utility function is defined in terms of the agent's history of interactions with its environment. This paper argues, via two examples, that the behavior problems can be avoided by formulating the utility function in two steps: 1) inferring a model of the environment from interactions, and 2) computing utility as a function of the environment model. Basing a utility function on a model that the agent must learn implies that the utility function must initially be expressed in terms of specifications to be matched to structures in the learned model. These specifications constitute prior assumptions about the environment so this approach will not work with arbitrary environments. But the approach should work for agents designed by humans to act in the physical world. The paper also addresses the issue of self-modifying agents and shows that if provided with the possibility to modify their utility functions agents will not choose to do so, under some usual assumptions.", "context": "As humans design increasingly complex AI agents, those agents will need to learn their world models rather than having pre-programmed world models. And rather than their actions being preprogrammed, the agents will require utility functions to motivate their actions. The contribution of this paper is to demonstrate how utility functions can be defined so that agents do not self-delude. A key difference is that such a model-based utility function must be defined in terms of both an agent's observations of the environment and its actions on the environment, since the environment model is learned from both observations and actions. This requires that utility function definitions be based on some assumptions about the environment. For example an agent designer may know that the environment contains certain kinds of objects and define the utility function in terms the states of those objects. The utility function may initially be defined by specifications that will match those objects and their states in the learned environment model. The specifications in an agent\u2019s utility function definition may fail to", "contribution": "This paper describes an approach to addressing these problems. A critical difference is that such a model-based utility function must be defined in terms of both an agent's observations of the environment and its actions on the environment, since the environment model is learned from both observations and actions. The contribution of this paper is to demonstrate how utility functions can be defined so that agents do not self-delude. This requires that utility function definitions be based on some assumptions about the environment.", "url": "https://doi.org/10.2478/v10229-011-0013-5", "title": "Model-based Utility Functions"}
{"distilbart": "Plantagora (Barthelson et al., 2011) is a web-based platform aimed at helping scientists view characteristics of the most popular sequencing platforms and assembly software for plant genomes. Plantagora has a well-designed interface to browse their database of evaluation results, but the results cannot be viewed through the friendly user interface; instead, the user has to parse a large log file. This leads to the questions of how to assess the quality of an assembly and how to compare different assemblies. Recently, there has been a lot of work on developing comprehensive ways to comparison different assemblers on four datasets. However, current sequencing technologies and software face many complications that impede reconstruction of full chromosomes, including errors in reads and large repeats in the genome. The DNA sequencing technologies cannot produce the complete sequence of a chromosome. Instead, they generate large numbers of reads, ranging from dozens to thousands of consecutive bases, sampled from different parts of the genome, resulting in many differences in the cont", "gold": "SUMMARY Limitations of genome sequencing techniques have led to dozens of assembly algorithms, none of which is perfect. A number of methods for comparing assemblers have been developed, but none is yet a recognized benchmark. Further, most existing methods for comparing assemblies are only applicable to new assemblies of finished genomes; the problem of evaluating assemblies of previously unsequenced species has not been adequately considered. Here, we present QUAST-a quality assessment tool for evaluating and comparing genome assemblies. This tool improves on leading assembly comparison software with new ideas and quality metrics. QUAST can evaluate assemblies both with a reference genome, as well as without a reference. QUAST produces many reports, summary tables and plots to help scientists in their research and in their publications. In this study, we used QUAST to compare several genome assemblers on three datasets. QUAST tables and plots for all of them are available in the Supplementary Material, and interactive versions of these reports are on the QUAST website. AVAILABILITY http://bioinf.spbau.ru/quast . SUPPLEMENTARY INFORMATION Supplementary data are available at Bioinformatics online.", "context": "BACKGROUND Genome assembly software combines the reads into larger regions called contigs. However, current sequencing technologies and software face many complications that impede reconstruction of full chromosomes, including errors in reads and large repeats in the genome. Different assembly programs use different heuristic approaches to tackle these challenges, resulting in many differences in the contig they output. This leads to the questions of how to assess the quality of an assembly and how to compare different assemblies. RESULTS We introduce QUAST (http://www.quast.org/), a new tool for assessing assembly quality. QUAST evaluates a full range of metrics needed by various users; however, the number of metrics is not so large that it would become difficult to interpret all of them. The interface and visualizations are easy to use, representative and informative.QUAST aggregates methods and quality metrics from existing software, such as Plantagora, GAGE, GeneMark.hmm and GlimmerHMM, and it", "contribution": "We introduce QUAST, a new tool for evaluating the quality of genome assemblies. QUAST aggregates methods and metrics from existing software, such as Plantagora, GAGE, GeneMark.hmm (Lukashin and Borodovsky 1998) and GlimmerHMM (Majoros et al., 2004), and it extends these with new metrics. For example, the well-known N50 statistic can be artificially increased by concatenating contigs, at the expense of increasing the number of misassemblies; QUAST introduces a new statistic, NA50, to counter this. The following metrics (except for NGx) can be evaluated with or without a reference genome. We also provide filtered versions of them, restricted to contigs of length above a specified minimum size, to exclude short contigs that may not be of much use.", "url": "https://doi.org/10.1093/bioinformatics/btt086", "title": "QUAST: quality assessment tool for genome assemblies."}
{"distilbart": "We present a new algorithm for appearance-preserving simplification. We convert our input surface to a decoupled representation. Surface position is represented in the typical way by a set of triangles with 3D coordinates stored at the vertices, respectively. Surface colors and normals are mapped to the surface with the aid of a surface parameterization, represented as 2D texture coordinates at the triangle vertices. The surface position is filtered using a standard surface approximation algorithm that makes local, complexity-reducing simplification operations (e.g., edge collapse, vertex removal), vertex removal, etc.). The color and normal attributes are filtered by the run-time system at the pixel level, using standard mip-mapping techniques [1]. Because the colors andnormals are now decoupling from the surface position, we employ a new texture deviation metric which effectively bounds the deviation of a mapped attribute value's position from its correct position on the original surface. We thus guarantee that each", "gold": "AbstractWe present a new algorithm for appearance-preserving simplification. Not only does it generate a low-polygon-count approximation of a model, but it also preserves the appearance. This is accomplished for a particular display resolution in the sense that we properly sample the surface position, curvature, and color attributes of the input surface. We convert the input surface to a representation that decouples the sampling of these three attributes, storing the colors and normals in texture and normal maps, respectively. Our simplification algorithm employs a new texture deviation metric, which guarantees that these maps shift by no more than a user-specified number of pixels on the screen. The simplification process filters the surface position, while the runtime system filters the colors and normals on a per-pixel basis. We have applied our simplification technique to several large models achieving significant amounts of simplification with little or no loss in rendering quality.", "context": "We present a new algorithm for appearance-preserving simplification of polygonal surfaces. We convert our input surface to a decoupled representation. Surface position is represented in the typical way, by a set of triangles with 3D coordinates stored at the vertices. Surface colors and normals are mapped to the surface with the aid of a surface parameterization, representing as 2D texture coordinates at the triangle vertexes. The surface position is filtered using a standard surface approximation algorithm that makes local, complexity-reducing simplification operations (e.g., edge collapse, vertex removal, etc.). The color and normal attributes are filtered by the run-time system at the pixel level, using standard mip-mapping techniques [1]. Because the colors and normsals are now decoupling from the surface position, we employ a new texture deviation metric, which effectively bounds the deviation of a mapped attribute value's position from its correct position on the original surface. We thus guarantee", "contribution": "We present a new algorithm for appearance-preserving simplification. We convert our input surface to a decoupled representation. Surface position is represented in the typical way, by a set of triangles with 3D coordinates stored at the vertices. The color and normal attributes are filtered by the run-time system at the pixel level, using standard mip-mapping techniques [1]. Because the colors and normals are now decoupling from the surface position, we employ a new texture deviation metric, which effectively bounds the deviation of a mapped attribute value's position from its correct position on the original surface. We thus guarantee that each attribute is appropriately sampled and mapped to screen-space. A particular point on the surface, with some color and some normal, may appear to shift by at most $\\varepsilon$ pixels on the screen.", "url": "https://doi.org/10.1145/280814.280832", "title": "Appearance-preserving simplification"}
{"distilbart": "Despite the increasing availability of techniques to measure even high-resolution temporal contact networks however, a number of limitations remain. In particular, measures cannot be carried out for arbitrarily large population sizes. It is thus of crucial interest to infer contacts or build contact proxies from data with lower spatial resolution data or coming from other sources. The resulting data is often temporally resolved and has been increasingly used in various contexts including the study of human behaviour, the validation of models of human interactions and data-driven models of epidemic spreading, or co-location at geographic scale. Other works try to infer close proximity in specific settings from individual attributes or from a very precise localisation of individuals, or, at geographical scale, from the similarity of the WiFi signals received from a large enough number of WiFi routers. Here instead, we do not attempt to infer specific contacts between pairs of individuals but rather investigate if a coarse co-presence information on individuals allows us to reach an overall picture of the contact patterns in the", "gold": "Technological advances have led to a strong increase in the number of data collection efforts aimed at measuring co-presence of individuals at different spatial resolutions. It is however unclear how much co-presence data can inform us on actual face-to-face contacts, of particular interest to study the structure of a population in social groups or for use in data-driven models of information or epidemic spreading processes. Here, we address this issue by leveraging data sets containing high resolution face-to-face contacts as well as a coarser spatial localisation of individuals, both temporally resolved, in various contexts. The co-presence and the face-to-face contact temporal networks share a number of structural and statistical features, but the former is (by definition) much denser than the latter. We thus consider several down-sampling methods that generate surrogate contact networks from the co-presence signal and compare them with the real face-to-face data. We show that these surrogate networks reproduce some features of the real data but are only partially able to identify the most central nodes of the face-to-face network. We then address the issue of using such down-sampled co-presence data in data-driven simulations of epidemic processes, and in identifying efficient containment strategies. We show that the performance of the various sampling methods strongly varies depending on context. We discuss the consequences of our results with respect to data collection strategies and methodologies.", "context": "We use data collected by the SocioPatterns collaboration in various contexts. These data were gathered using wearable sensors able to detect face-to-face close range proximity (1.5 m) of participants wearing the sensors on their chests. In addition, the sensors broadcast a signal that can be received by RFID readers located in the environment. In open space, each reader can receive signals from sensors situated within a range of \u223c30 m, while the actual reception range in a building depends on its specific structure and on the nature of its walls, floors and ceilings. Each reader thus defines a coarse spatial area and the sensors' signals can be followed when the individuals carrying them change area. For each sensor, we define its \"spatial location\" at each time as the set of readers receiving its broadcasted signal at this time, i.e., the same exact set of readings have received signals from both individuals. We first compare the structural and statistical properties of these two temporal networks", "contribution": "In this work, we leverage several data sets collected by the SocioPatterns collaboration in various contexts: these data include both detailed information about close, face-to-face encounters between individuals and a location tracking of individuals with low spatial resolution. We first compare the structural and statistical properties of these two temporal networks and show that they share some important properties, although the co-presence network is much denser, due to the lower spatial resolution involved in its definition. We thus investigate several methods of down-sampling the co\u2010presence signal in order to create surrogate contact networks, in the spirit of [28, 29], and compare these surrogate data to the actual networks of face\u2010to\u2010face contacts. We focus first on several statistical characteristics of temporal and aggregated networks, and quantify the ability to identify central nodes in the contact network from the surrogate data. We then consider the possibility to use the surrogate\u2010data in numerical simulations of data\u2010driven models for epidemic spread", "url": "https://doi.org/10.1140/epjds/s13688-018-0140-1", "title": "Can co-location be used as a proxy for face-to-face contacts?"}
{"distilbart": "This paper analyses a non-elitist, population based EA that uses linear ranking selection and bit-wise mutation with stochastic selection. The main contributions are an analysis of situations where the mutation-selection balance has an exponentially large impact on the runtime, and new techniques based on branching processes for analysing non-Elitist population based EAs. They found the expected runtime on these fitness functions to be the same for all three selection mechanisms. None of the results above show how the balance between the selection pressure and mutation rate impacts the runtime. However, they pointed out that polynomial runtime can be achieved by scaling the fitness function. Witt also studied a population-based algorithm with fitness proportionate selection, however with the objective to study the role of populations.", "gold": "The interplay between mutation and selection plays a fundamental role in the behaviour of evolutionary algorithms (EAs). However, this interplay is still not completely understood. This paper presents a rigorous runtime analysis of a non-elitist population-based EA that uses the linear ranking selection mechanism. The analysis focuses on how the balance between parameter $\\eta$, controlling the selection pressure in linear ranking, and parameter $\\chi$ controlling the bit-wise mutation rate, impacts the runtime of the algorithm. The results point out situations where a correct balance between selection pressure and mutation rate is essential for finding the optimal solution in polynomial time. In particular, it is shown that there exist fitness functions which can only be solved in polynomial time if the ratio between parameters $\\eta$ and $\\chi$ is within a narrow critical interval, and where a small change in this ratio can increase the runtime exponentially. Furthermore, it is shown quantitatively how the appropriate parameter choice depends on the characteristics of the fitness function. In addition to the original results on the runtime of EAs, this paper also introduces a very useful analytical tool, i.e., multi-type branching processes, to the runtime analysis of non-elitist population-based EAs.", "context": "Evolutionary algorithms (EAs) have been applied successfully to many optimisation problems. However, despite several decades of research, many fundamental questions about their behaviour remain open. One of the central questions regarding EAs is to understand how to define such trade-off quantitatively and how to achieve it. This paper analyses rigorously a non-elitist, population based EA that uses linear ranking selection and bit-wise mutation. The main contributions are an analysis of situations where the mutation-selection balance has an exponentially large impact on the runtime, and new techniques based on branching processes for analysing non-Elitist population based EAs.", "contribution": "This paper analyses the runtime of a non-elitist, population based EA that uses linear ranking selection and bit-wise mutation. The main contributions are an analysis of situations where the mutation-selection balance has an exponentially large impact on the runtime, and new techniques based on branching processes for analysing population based EAs. This paper significantly extends this early work. In addition to strengthening the main result, simplifying several proofs and proving a conjecture, we have added a completely new section that introduces multi-type branching processes as an analytical tool for studying the runtime.", "url": "https://doi.org/10.1145/1527125.1527133", "title": "On the Impact of Mutation-Selection Balance on the Runtime of Evolutionary Algorithms"}
{"distilbart": "Gene regulation is one of the fundamental mechanisms by which an organism responds to external and internal biochemical signals and facilitates differentiation and important developmental events. The importance of understanding the mechanisms of gene regulation has been well known in life sciences. One of the theories for the Cambrian explosion is that the developments of new regulatory networks lead to prototypes of virtually all-modern species. Innovative experimental techniques like Microarrays (Bucher, 1999;  http://cmgm.stanford.edu/pbrown/sporulation) have provided a flood of data that can be mined for conserved regulatory motifs within a genome and between species. Methods to detect such elements embedded in non-coding sequences are bound to accelerate our understanding of transcription factors and the vital role they play in complex organisms. There have been two approaches for finding binding sites. One approach is based on dictionary building or word counting (Jensen and Knudsen, 2000; van Helden et al., 1998; Simon", "gold": "MOTIVATION Experimental methods capable of generating sets of co-regulated genes have become commonplace, however, recognizing the regulatory motifs responsible for this regulation remains difficult. As a result, computational detection of transcription factor binding sites in such data sets has been an active area of research. Most approaches have utilized either Gibbs sampling or greedy strategies to identify such elements in sets of sequences. These existing methods have varying degrees of success depending on the strength and length of the signals and the number of available sequences. We present a new deterministic iterative algorithm for regulatory element detection based on a Markov chain background. As in other methods, sequences in the entire genome and the training set are taken into account in order to discriminate against commonly occurring signals and produce patterns, which are significant in the training set. RESULTS The results of the algorithm compare favorably with existing tools on previously known and newly compiled data sets. The iteration based search appears rather rigorous, not only finding the binding sites, but also showing how the binding site stands out from genomic background. The approach used to score the results is critical and a discussion of various scoring schemes and options is also presented. Benchmarking of several methods shows that while most tools are good at detecting strong signals, Gibbs sampling algorithms give inconsistent results when the regulatory element signal becomes weak. A Markov chain based background model alleviates the drawbacks of MAP (maximum a posteriori log likelihood) scores. AVAILABILITY Available on request from the authors. SUPPLEMENTARY INFORMATION Data and the results presented in this paper are available on the web at http://compbio.ornl.gov/mira/index.html", "context": "With the annotation of many genomes, the stage is set for understanding the complex inter-relationships between genes and the ways in which they act in concert. In recent years, with new experimental tools for studying gene expression, more attention is being paid to the information present in the non-coding regions of the genome, particularly the regulatory regions. Gene regulation is one of the fundamental mechanisms by which an organism responds to external and internal biochemical signals and facilitates differentiation and important developmental events. The importance of understanding the mechanisms of gene regulation has been well known in life sciences. One of the theories for the Cambrian explosion is that the developments of new regulatory networks lead to prototypes of virtually all-modern species. Innovative experimental techniques like Microarrays (Bucher, 1999; http://cmgm.stanford.edu/pbrown/sporulation/) have provided a flood of data that can be mined for conserved regulatory motifs within a species and between species. Methods to", "contribution": "In this paper a new iterative algorithm called MIRA (Markov-background Iterative Rareness Algorithm) is presented and evaluated using existing microarray data, as well as data from the Saccharomyces cerevisiae Promoter Database. Its performance is compared with existing methods. A discussion of advantages and disadvantages of various scoring scheme is also presented. From a given set of genes, the goal is to find the subset of genes that are likely to be co-regulated and the likely binding site responsible for this observed co-regulation.", "url": "https://doi.org/10.1093/bioinformatics/btg266", "title": "Background rareness-based iterative multiple sequence alignment algorithm for regulatory element detection."}
{"distilbart": "In this paper, we propose an approach that attempts to provide this control and streamlines video generation by using captions. To create a frame based on a caption, instead of trivially conditioning the generation on the caption text, our network introduces a soft-attention over the captions separately for the long-term and short-term context of the video; (2) A methodology for selectively combining information for conditioning at various levels of the architecture using appropriate attention mechanisms; and (3) A network architecture which learns a robust latent representation of videos and is able to perform competently on tasks such as unsupervised video generation and action recognition. The results obtained on standard datasets that earlier efforts have used for such efforts are very promising, and support the claims of this work. Recent methods have enabled learning a much more robust latent representations of the given data [12], 13, 3, 30] and are helping improving the performance of supervised learning tasks.", "gold": "This paper proposes a network architecture to perform variable length semantic video generation using captions. We adopt a new perspective towards video generation where we allow the captions to be combined with the long-term and short-term dependencies between video frames and thus generate a video in an incremental manner. Our experiments demonstrate our network architecture's ability to distinguish between objects, actions and interactions in a video and combine them to generate videos for unseen captions. The network also exhibits the capability to perform spatio-temporal style transfer when asked to generate videos for a sequence of captions. We also show that the network's ability to learn a latent representation allows it generate videos in an unsupervised manner and perform other tasks such as action recognition. (Accepted in International Conference in Computer Vision (ICCV) 2017)", "context": "In this work, we propose an approach that attempts to streamlines video generation by using captions. In order to generate such frames, we follow a recurrent attentive approach similar to [3], which focuses on one part of the frame at each time step for generation, and completes the frame generation over multiple time steps. By iteratively generating the frame over a number of time-steps in response to a given caption, our network adds caption-driven semantics to the generated video. A key advantage of following such an approach is the possibility to generate videos with multiple captions and thus change the contents of the video midway according to the new caption. To create a frame based on a caption, instead of trivially conditioning the generation on the caption text, we introduce a soft-attention over the captions separately for the long-term and short-term contexts. This attention mechanism serves the crucial role of allowing the network to selectively combine information for conditioning at various levels of the architecture using appropriate", "contribution": "In this work, we propose an approach that attempts to provide this control and streamlines video generation by using captions. In order to generate such frames, we follow a recurrent attentive approach similar to [3], which focuses on one part of the frame at each time step for generation, and completes the frame generation over multiple time steps. By iteratively generating the frame over a number of time-steps in response to a given caption, our network adds caption-driven semantics to the generated video. A key advantage of following such an approach is the possibility to generate videos with multiple captions and thus change the contents of the video midway according to the new caption. To create a frame based on a caption, instead of trivially conditioning the generation on the caption text, we introduce a soft-attention over the captions separately for the long-term and short-term contexts. This attention mechanism serves the crucial role of allowing the network to selectively combine the various parts of the caption with", "url": "https://doi.org/10.1109/ICCV.2017.159", "title": "Attentive Semantic Video Generation using Captions"}
{"distilbart": "Generative Adversarial Network (GAN) is a family of nonparametric density estimation models which learn to model the data generating distribution using adversarial training. We show that the proposed Multi-Discriminator CycleGAN, without pretraining discriminators with pretrained discriminator for spectrogram adaptation, outperforms CycleGAN on parallel domains. Furthermore, we demonstrate that the multi discriminator architecture can overcome the checkerboard artifacts problem caused by deconvolution layer in generator and generates natural clean audio. To evaluate the performance of the proposed model, gender-based domain adaptations are selected as domain adaptations. In this paper, we propose to use multiple and independent discriminators for each domain, similar to generative multi adversarial networks (GMAN). We also show that GAN outperform CycleGAN with pretrain discriminator, for Spectrogram adaptation.", "gold": "Domain adaptation plays an important role for speech recognition models, in particular, for domains that have low resources. We propose a novel generative model based on cyclic-consistent generative adversarial network (CycleGAN) for unsupervised non-parallel speech domain adaptation. The proposed model employs multiple independent discriminators on the power spectrogram, each in charge of different frequency bands. As a result we have 1) better discriminators that focus on fine-grained details of the frequency features, and 2) a generator that is capable of generating more realistic domain-adapted spectrogram. We demonstrate the effectiveness of our method on speech recognition with gender adaptation, where the model only has access to supervised data from one gender during training, but is evaluated on the other at test time. Our model is able to achieve an average of $7.41\\%$ on phoneme error rate, and $11.10\\%$ word error rate relative performance improvement as compared to the baseline, on TIMIT and WSJ dataset, respectively. Qualitatively, our model also generates more natural sounding speech, when conditioned on data from the other domain.", "context": "In this paper, we propose a new generative model based on CycleGAN for unsupervised domain adaptation. The generator should be back-propagated with multiple gradient signals (from different discriminators), that each represents the variations between source and target domains at a specific frequency band. To achieve this goal, it is imperative that CycleGAN correctly catch the spectro-temporal variations between different frequency bands across domains during training. This will allow the generator to learn the mapping function which can convert spectrogram from source to target domain. We show that the proposed Multi-Discriminator CycleGAN without pretraining the discriminators outperforms CycleGAN with pretrained discriminator, for spectrogram adaptation. Furthermore, we show that multi discriminator architecture can overcome the checkerboard artifacts problem caused by deconvolution layer in generator, and generates natural clean audio.", "contribution": "In this paper, we propose a new generative model based on CycleGAN for unsupervised domain adaptation. The generator should be back-propagated with multiple gradient signals (from different discriminators), that each represents the variations between source and target domains at a specific frequency band. To achieve this goal, it is imperative that CycleGAN correctly catch the spectro-temporal variations between different frequency bands across domains during training. This will allow the generator to learn the mapping function which can convert spectrogram from source to target domain. We show that the proposed Multi-Discriminator CycleGAN, without pretraining the discriminators, outperforms CycleGAN with pretrained discriminator, for spectrogram adaptation. Furthermore, we show that multi discriminator architecture can overcome the checkerboard artifacts problem caused by deconvolution layer in generator, and generates natural clean audio.", "url": "https://doi.org/10.21437/Interspeech.2018-1535", "title": "A Multi-Discriminator CycleGAN for Unsupervised Non-Parallel Speech Domain Adaptation"}
{"distilbart": "In PKCS #11, a token is used as an interface between applications and (portable) cryptographic devices. This is achieved through the use of a personal identification number (PIN), which acts essentially as a password. The standard allows for this mechanism to be augmented with or replaced by an alternative, custom mechanisms in any given implementation (e.g., PINpad or smarts cards). This does not prevent access to other users' token objects although this could be made another implementation feature. The standards cover a variety of aspects of Public Key Cryptography Standards (PKCS) including RSA Encryption Standard, PKCS#11: Cryptographic Token Interface Standard [18] and PKCS_8: Private-Key Information Syntax Standard. Many significant APIs and protocols have been built upon PKCS (#11) (including Mozilla (the open source browser on which the Netscape browser is based) and SSL hardware accelerators from companies such as nCipher, IBM, Thales,", "gold": "Abstract. Public Key Cryptography Standards (PKCS) #11 has gained wide acceptance within the cryptographic security device community and has become the interface of choice for many applications. The high esteem in which PKCS #11 is held is evidenced by the fact that it has been selected by a large number of companies as the API for their own devices. In this paper we analyse the security of the PKCS #11 standard as an interface (e.g. an application-programming interface (API)) for a security device. We show that PKCS #11 is vulnerable to a number of known and new API attacks and exhibits a number of design weaknesses that raise questions as to its suitability for this role. Finally we present some design solutions.", "context": "The Public Key Cryptography Standards (PKCS) were developed by RSA Security Inc. \"in cooperation with representatives of industry, academia and government to provide a standard to allow interoperability and compatibility between vendor devices and implementations.\"1 A significant factor in the success of these standards can be attributed to this co-operative approach. The standards cover a variety of aspects of Public Key cryptography including PKCS #1: RSA Encryption Standard, PKCS#11: Cryptographic Token Interface Standard [18] and PKCS (#8: Private-Key Information Syntax Standard). Many significant APIs and protocols have been built upon PKCS_11 (e.g. SSL). Notable products with PKCS support include Mozilla (the open source browser upon which the Netscape browser is based) and SSL hardware accelerators from companies such as nCipher, IBM, Thales, Rainbow and AEP amongst others. Indeed, this research was prompted by the question of the suitability of the PK", "contribution": "In this paper, we describe the design and implementation of PKCS #11. We discuss several areas of concern including operating system security, the actions of rogue applications and the threat posed by Trojan linked libraries or device drivers that may subvert security, perhaps by stealing the secret values thereof.", "url": "https://doi.org/10.1007/978-3-540-45238-6_32", "title": "On the Security of PKCS #11"}
{"distilbart": "The purpose of this paper is twofold. First, we aim to analyze the opportunities, limitations, and challenges of a system that allows activist groups to try different strategies for calling volunteers to action. Second, we focus on understanding how social media users respond to messages (strategies) when they are delivered by an online bot or individual, and not addressed how people respond when the messages are presented by the topic of activism. In addition, an effort is made to understand the communication patterns of the people who decide to engage with Botivist. This analysis is important as these individuals could become one core day. For this purpose, we designed and conducted experiments on Twitter in order to test what is the most effective strategy. All of this hampers activists' success. To help activists identify the best strategies, we present Botivist, a platform that, by leveraging online bots, allows Activist Groups to attempt different strategies to trigger contributions from volunteers.", "gold": "To help activists call new volunteers to action, we present Botivist: a platform that uses Twitter bots to find potential volunteers and request contributions. By leveraging different Twitter accounts, Botivist employs different strategies to encourage participation. We explore how people respond to bots calling them to action using a test case about corruption in Latin America. Our results show that the majority of volunteers (>80%) who responded to Botivist's calls to action contributed relevant proposals to address the assigned social problem. Different strategies produced differences in the quantity and relevance of contributions. Some strategies that work well offline and face-to-face appeared to hinder people's participation when used by an online bot. We analyze user behavior in response to being approached by bots with an activist purpose. We also provide strong evidence for the value of this type of civic media, and derive design implications.", "context": "Activist groups usually have a small set of highly motivated core members who give a great deal of their own time and resources to make change in the world. However, for years, activists went door-to-door to recruit and engage casual volunteers. Recently, new technologies have helped activists build the support of casual volunteers [5, 16]. Some use mailing lists to maintain continuous communication with their volunteers [23, 29]. Others are using social media [34], but despite the technological advancements, social computing has not been widely employed to connect casual volunteers[35]. Many eager individuals receive little direction on how to help [30]. Activist groups must still spend time figuring out how they will present their campaigns in order to successfully trigger action [34]. Understanding how the presentation of a campaign (message) affects the engagement of volunteers and people in general has been extensively studied in both theories \u2026", "contribution": "To help activists identify the best strategies, we present Botivist, a platform that, by leveraging online bots, allows activist groups to try different strategies for calling volunteers to action. The platform then tries different strategies to request contributions on social media, and helps advance the group's plans. Activist groups thus receive help about how to prompt contributions from strangers, reducing the need to invest time in people who might never contribute. We analyze the opportunities, limitations, and challenges of a system that uses online bots to call people to action in order to act upon social issues. Secondly, we analyze the online behaviours of those who are most likely to respond to activist bots.", "url": "https://doi.org/10.1145/2818048.2819985", "title": "Botivist: Calling Volunteers to Action Using Online Bots"}
{"distilbart": "Ontology is one strategy for promoting interoperability of data, and ontologies have been created for a wide variety of biological and clinical domains, including genes and gene products, proteins and protein modifications, and in many other areas of biology and biomedicine. The provision of natural language definitions for ontology terms enables the content of the ontology to be evaluated by human experts and to keep pace with the advance of science. In this way it also allows a plurality of ontologies to be combined together and the resulting ontology can be checked for consistency in its turn. Thus for example, the relation output_of between imaging process and image, and the relation about between cell images and staining (some background and cellular imaging) might be combined with ontologies provided in [10]. In our case, the use of the Gene Ontology (GO), initially in annotating experimental data and literature in a variety of omics disciplines, and now increasingly in clinical fields. This occurred initially", "gold": "BACKGROUND Ontology is one strategy for promoting interoperability of heterogeneous data through consistent tagging. An ontology is a controlled structured vocabulary consisting of general terms (such as \"cell\" or \"image\" or \"tissue\" or \"microscope\") that form the basis for such tagging. These terms are designed to represent the types of entities in the domain of reality that the ontology has been devised to capture; the terms are provided with logical definitions thereby also supporting reasoning over the tagged data. AIM This paper provides a survey of the biomedical imaging ontologies that have been developed thus far. It outlines the challenges, particularly faced by ontologies in the fields of histopathological imaging and image analysis, and suggests a strategy for addressing these challenges in the example domain of quantitative histopathology imaging. RESULTS AND CONCLUSIONS The ultimate goal is to support the multiscale understanding of disease that comes from using interoperable ontologies to integrate imaging data with clinical and genomics data.", "context": "An ontology is one strategy for promoting interoperability of data, and ontologies have been created for a wide variety of biological and clinical domains. These terms are organized in a graph-theoretical hierarchy ordered by subtype relations, and this hierarchy constitutes what we can think of as the taxonomical backbone of the ontology. The ontology will include also assertions of a range of different sorts of relations between the entities represented by its terms in addition to the subtype relation, which can be represented by means of further edges in the graph. Such relations will include, for example, the relation output_of between imaging process and image, and the relation about between cell image and cell. Ontologies are designed to be human readable and also to allow automated reasoning over the domain. The terms themselves are selected and formulated in such a way as to be as close as possible to established terminological usage within the relevant discipline\u2014and the first step in building an ontologia is often", "contribution": "In this paper, we provide a brief overview of the ontology and its use in image-related fields. We then discuss how OWL can be used to create an ontology that is human readable and allows for automated reasoning over the domain. The ontology will include also assertions of a range of different sorts of relations between the entities represented by its terms in addition to the subtype relation, which can be represented by means of further edges in the graph. Such relations will include, for example, the relation output_of between imaging process and image, and the relation about between cell image and cell.", "url": "https://doi.org/10.4103/2153-3539.159214", "title": "Biomedical imaging ontologies: A survey and proposal for future work"}
{"distilbart": "In this paper, we study the relation between various declarative problem-solving domains concerned with dynamic systems and identify which inferences are studied in which domains are studied. We show that progression on the LTCtheory can be performed by model expansion on the transition theory and that simulation can be achieved by repeated progression. Proving invariants is achieved by induction: by proving it for the initial theory and proving that the property is preserved by the transformation theory. Finally we discuss how one could handle more complex dynamic properties.", "gold": "To appear in Theory and Practice of Logic Programming (TPLP). Dynamic systems play a central role in fields such as planning, verification, and databases. Fragmented throughout these fields, we find a multitude of languages to formally specify dynamic systems and a multitude of systems to reason on such specifications. Often, such systems are bound to one specific language and one specific inference task. It is troublesome that performing several inference tasks on the same knowledge requires translations of your specification to other languages. In this paper we study whether it is possible to perform a broad set of well-studied inference tasks on one specification. More concretely, we extend IDP3 with several inferences from fields concerned with dynamic specifications.", "context": "In this paper, we evaluate the usefulness of the Knowledge Base System (KBS) paradigm in the well-studied domain of dynamic action languages. We identify many interesting inference tasks in this domain and we show that for one concrete action language, the Linear Time Calculus (LTC)-theory introduced in this paper can be performed. As a result, we can use the same specification, an LTC-theory, for performing a wide range of tasks, whereas traditional software development uses different specifications for different tasks. We illustrate this with different tasks related to development of a PacMan game.", "contribution": "In this paper, we evaluate the usefulness of the Knowledge Base System (KBS) paradigm in the well-studied domain of dynamic action languages. We identify many interesting inference tasks in this domain and we show that for one concrete action language, the Linear Time Calculus (LTC)-theory introduced in this paper can be performed. As a result, we can use the same specification, an LTC-theory, for performing a wide range of tasks, whereas traditional software development uses different specifications for different tasks. We illustrate this with different tasks related to development of a Pac-Man game.", "url": "https://doi.org/10.1017/S1471068414000155", "title": "Simulating dynamic systems using Linear Time Calculus theories"}
{"distilbart": "The development of drug resistance is one of the main reasons for the need for novel drugs against Mtb, and thus understanding and predicting the functional effect of polymorphisms on their targets is of prime importance in their development. As with modelling, computational methods are increasingly able to step up and provide insight where more expensive experimental methods are unable to, with programs such as SIFT (SIFT), Site Directed Mutator (SDM; 12, 13) and PolyPhen (14) being developed in recent years. In this work, we present CHOPIN, a database built on an automated, high-throughput modelling pipeline using multiple templates, annotated according to functional state. This is of relevance since much of the redundancy of sequences in the PDB is due to the study of multiple forms of proteins, which manifest in conformational differences according to context. While some of these might be too subtle to exert meaningful influence on a homology model, others can be quite drastic so", "gold": "Tuberculosis kills more than a million people annually and presents increasingly high levels of resistance against current first line drugs. Structural information about Mycobacterium tuberculosis (Mtb) proteins is a valuable asset for the development of novel drugs and for understanding the biology of the bacterium; however, only about 10% of the \u223c4000 proteins have had their structures determined experimentally. The CHOPIN database assigns structural domains and generates homology models for 2911 sequences, corresponding to \u223c73% of the proteome. A sophisticated pipeline allows multiple models to be created using conformational states characteristic of different oligomeric states and ligand binding, such that the models reflect various functional states of the proteins. Additionally, CHOPIN includes structural analyses of mutations potentially associated with drug resistance. Results are made available at the web interface, which also serves as an automatically updated repository of all published Mtb experimental structures. Its RESTful interface allows direct and flexible access to structures and metadata via intuitive URLs, enabling easy programmatic use of the models.", "context": "BACKGROUND The Mycobacterium tuberculosis (Mtb) proteome is one of the major challenges facing the global fight against tuberculosis, which requires a better identification and understanding of potential molecular targets. In this work, we present CHOPIN, a database built on an automated, high-throughput modelling pipeline using multiple templates, annotated according to functional state. METHODOLOGY/PRINCIPAL FINDINGS We have developed a database of template libraries from the H37Rv reference genome, as obtained from the Tuberculosis Database (TBDB). It outputs a set of models and alignments, together with a relational database of the data necessary for the web interface. An up-to-date compendium of Mtb structures has been made easily accessible from a web interface at http://structure.bioc.uk/toccata. The database incorporates all domains from SCOP 1.75A and CATH1.75B in conjunction", "contribution": "In this work, we present CHOPIN (http://structure.bioc.uk/toccata), a database built on an automated, high-throughput modelling pipeline using multiple templates for Mycobacterium tuberculosis (Mtb) proteomes and annotated according to functional state. The database also incorporates an analysis of polymorphisms that are possibly related to drug resistance. All the information, together with an up-to-date compendium of Mtb structures, is made easily accessible from a web interface.", "url": "https://doi.org/10.1093/database/bav026", "title": "CHOPIN: a web resource for the structural and functional proteome of Mycobacterium tuberculosis"}
{"distilbart": "In this paper, we consider the disambiguation of the sense \"gene\" or \"protein\" when the name is not disambigenuated explicitly by the author with the word \"Gene\" (e.g. \"SBP2 gene\"). This task is important because it has made it difficult to find information from literature specifically for proteins and the corresponding genes. However, database searches provide a lot of hits among which the correct and important articles have to be sorted manually. Therefore, for example, in data mining related to proteomics the scientists could save much time if they could direct their search only to proteins. In contrast, some other gene/protein names do not retain their ambiguity in the sublanguage: (1) Expression of either BZLF1 or BRLF1 triggers expression of... (2)... DNA in lymphoblastoid cell lines induced by transfection with BZ-LCL targets was associated with the abundance of BZ", "gold": "BackgroundThe ability to distinguish between genes and proteins is essential for understanding biological text. Support Vector Machines (SVMs) have been proven to be very efficient in general data mining tasks. We explore their capability for the gene versus protein name disambiguation task.ResultsWe incorporated into the conventional SVM a weighting scheme based on distances of context words from the word to be disambiguated. This weighting scheme increased the performance of SVMs by five percentage points giving performance better than 85% as measured by the area under ROC curve and outperformed the Weighted Additive Classifier, which also incorporates the weighting, and the Naive Bayes classifier.ConclusionWe show that the performance of SVMs can be improved by the proposed weighting scheme. Furthermore, our results suggest that in this study the increase of the classification performance due to the weighting is greater than that obtained by selecting the underlying classifier or the kernel part of the SVM.", "context": "BACKGROUND The amount of scientific biomedical literature readable by computer programs is overwhelming. Therefore automatic literature-mining methods can be exploited in order to retrieve relevant information (for recent thorough reviews of related work in BioNLP, see e.g. [2, 3]. For example, several algorithms have been developed for extracting information about protein\u2013protein interactions from the biomedical literature [4, 5]. In this paper, we consider the disambiguation of the sense \"gene\" or \"protein\" when the name is not disambibed explicitly by the author with the word \"genes\" or proteins (e.g., SBP2 gene). This task is important, because the release of the human genome and large scale functional genomics studies and methods have made it important to be able to find information from literature specifically for proteins and the corresponding genes. However, database searches provide a lot of hits among which the correct and important articles have to be sorted manually. Therefore", "contribution": "In this paper, we consider the disambiguation of the sense \"gene\" or \"protein\" when the name is not misambiguated explicitly by the author with the word\"gene\", or \"proteins\". The three experts unanimously agreed only in 78% of the cases, each name being classified as either a gene, protein or mRNA. This low rate of inter-annotator agreement suggests that the task is relatively difficult even for human experts, reflecting the inherent complexity of the domain. However, the study does not analyse more closely the reasons that lead to annotation disagreements. In this study, a domain corpus was annotated by three biology experts. CONCLUSIONS The results show that there exist Drosophila gene names such as ring and arc that can be confused with their ordinary meanings. Manual analysis of a small set of abstracts returned by PubMed for the query ring and drosophila shows that the word ring appears in its gene/protein sense", "url": "https://doi.org/10.1186/1471-2105-6-157", "title": "Contextual weighting for Support Vector Machines in literature mining: an application to gene versus protein name disambiguation"}
{"distilbart": "A great amount of effort has gone into the implementation of linear algebra libraries. However, there is little software that can take advantage of the large-scale heterogeneous systems efficiently, especially to utilize all CPU cores and all GPUs. Considering many operations of scientific computing applications are carried out through numerical linear algebra library, we focus on providing fundamental linear algebra algorithms on the new heterogeneous architectures. Our solution consists of three essential components: (1) a static multi-level data distribution method, (2) heterogeneous tile algorithms, and (3) a distributed dynamic scheduling runtime system. The solution works as follows. Given a matrix input, we first split it into tiles of hybrid sizes. Then we distribute the tiles to the hosts' main memories and the GPU device memories on a cluster with a static method. Each compute node runs a runtime system (launched as an MPI process) that schedules tasks within the node dynamically. Different nodes communicate with each other by means of MPI messages.", "gold": "GPU-based heterogeneous clusters continue to draw attention from vendors and HPC users due to their high energy efficiency and much improved single-node computational performance, however, there is little parallel software available that can utilize all CPU cores and all GPUs on the heterogeneous system efficiently. On a heterogeneous cluster, the performance of a GPU (or a compute node) increases in a much faster rate than the performance of the PCI-Express connection (or the interconnection network) such that communication eventually becomes the bottleneck of the entire system. To overcome the bottleneck, we developed a multi-level partitioning and distribution method that guarantees a near-optimal communication volume. We have also extended heterogeneous tile algorithms to work on distributed memory GPU clusters. Our main idea is to execute a serial program and generate hybrid-size tasks, and follow a dataflow programming model to fire the tasks on different compute nodes. We then devised a distributed dynamic scheduling runtime system to schedule tasks, and transfer data between hybrid CPU-GPU compute nodes transparently. The runtime system employs a novel distributed task-assignment protocol to solve data dependencies between tasks without coordination between processing units. The runtime system on each node consists of a number of CPU compute threads, a number of GPU compute threads, a task generation thread, an MPI communication thread, and a CUDA communication thread. By overlapping computation and communication through dynamic scheduling, we are able to attain a high performance of 75 TFlops for Cholesky factorization on the heterogeneous Keeneland system using 100 nodes, each with twelve CPU cores and three GPUs. Moreover, our framework is able to attain high performance on distributed-memory clusters without GPUs, and shared-system multiGPUs.", "context": "In this work, we present a unified framework to solve linear algebra problems on any number of CPU cores and GPUs. Our solution consists of three essential components: (1) static multi-level data distribution method, (2) heterogeneous tile algorithms, and (3) distributed dynamic scheduling runtime system. The solution works as follows. Given a matrix input, we first split it into tiles of hybrid sizes. Then we distribute the tiles to the hosts' main memories and the GPU device memories on a cluster with a static method. Each compute node runs a runtime system (launched as an MPI process) that schedules tasks within the node dynamically. Different nodes communicate with each other by means of MPI messages. Our runtime system follows the data-flow programming model and builds a partial directed acyclic graph (DAG) dynamically, where a completed task will trigger a set of new tasks in the DAG. On the top level, we use a 2-D block cycl", "contribution": "In this work, we present a unified framework to solve linear algebra problems on any number of CPU cores and GPUs. Our solution consists of three essential components: (1) static multi-level data distribution method, (2) heterogeneous tile algorithms, and (3) distributed dynamic scheduling runtime system for heterogeneous clusters with hybrid CPUs/GPUs. Given a matrix input, we first split it into tiles of hybrid sizes. Then we distribute the tiles to the hosts' main memories and the GPU device memories on a cluster with a static method. Each compute node runs a runtime system (launched as an MPI process) that schedules tasks within the node dynamically. Different nodes communicate with each other by means of MPI messages. Our runtime system follows the data-flow programming model and builds a partial directed acyclic graph (DAG) dynamically, where a completed task will trigger a set of new tasks in the DAG. On the top (i.e., inter", "url": "https://doi.org/10.1145/2312005.2312025", "title": "A scalable framework for heterogeneous GPU-based clusters"}
{"distilbart": "The unprecedented prosperity in epigenetics studies of DNA and histone modifications with next-generation sequencing data (Bernstein et al., 2007; Thurman et al, 2012) has not benefitted as much from the advancement in sequencing technology until lately. Two recent studies of transcriptomewide mRNA m6A methylation have proposed a new protocol 'FRIP-Seq', which stands for \u2018Fragmented RNA ImmunoPrecipitation Sequencing\u2019 and \u2018MeRIP\u2013Seq\u2019. This protocol in theory enabled the transcriptome-wide unbiased study of a repertoire of 4100 known post-transcriptional RNA modifications at a near single-base resolution, provided the corresponding antibody is available. To unify the nomenclatures, we call this protocol \u201cFRIP\u2013SeQ\u201d, where mRNA is fragmented before the immunoprecipitation with anti-m6 A antibody, and the immunopsipitated and input control fragments are", "gold": "MOTIVATION Fragmented RNA immunoprecipitation combined with RNA sequencing enabled the unbiased study of RNA epigenome at a near single-base resolution; however, unique features of this new type of data call for novel computational techniques. RESULT Through examining the connections of RNA epigenome sequencing data with two well-studied data types, ChIP-Seq and RNA-Seq, we unveiled the salient characteristics of this new data type. The computational strategies were discussed accordingly, and a novel data processing pipeline was proposed that combines several existing tools with a newly developed exome-based approach 'exomePeak' for detecting, representing and visualizing the post-transcriptional RNA modification sites on the transcriptome. AVAILABILITY The MATLAB package 'exomePeak' and additional details are available at http://compgenomics.utsa.edu/exomePeak/.", "context": "Despite the unprecedented progress in epigenetics studies of DNA and histone modifications with next-generation sequencing data, the RNA epigenetics remains a largely uncharted territory. Two recent studies of transcriptome-wide mRNA m6A methylation proposed a new powerful protocol (differently named as \u2018m6A-Seq\u2019 and \u2018MeRIP\u2013Seq'), where mRNA is fragmented before the immunoprecipitation with anti-m6 A antibody, and the immunoprecipitated and input control fragments are then sequenced for reconstructing transcriptome\u2010wide methylation sites. This protocol in theory enabled the transcriptome\u2011wide unbiased study of a repertoire of 4100 known post\u2010transcriptional RNA modifications at a near single\u2010base resolution, provided the corresponding antibody is available. To unify the nomenclatures, we call this protocol \u2018FRIP\u2010Seq', which stands for \u2018Fragmented RNA ImmunoPrecipitation", "contribution": "This protocol in theory enabled the transcriptome-wide unbiased study of a repertoire of 4100 known post-transcriptional RNA modifications (Cantara et al., 2011) at a near single-base resolution, provided the corresponding antibody is available. To unify the nomenclatures, we call this protocol 'FRIP-Seq', which stands for \u2018Fragmented RNA ImmunoPrecipitation Sequencing\u2019'. Next, we discuss briefly the best practice for FRIP\u2013Seq data analysis.", "url": "https://doi.org/10.1093/bioinformatics/btt171", "title": "Exome-based analysis for RNA epigenome sequencing data."}
{"distilbart": "We show that the Kuramoto system is a model of many physical, biological and chemical systems. To test the inference method under controllable experimental conditions, we also consider R\u00f6ssler oscillators, operating in the chaotic regime, which are implemented electronically. I find that (i) under appropriated coupling conditions the network can be perfectly inferred, these conditions being a weak coupling regime where the network is neither fully synchronized nor completely desynchronized and (ii) regarding the statistical similarity measure used for inferring the network structural connectivity, the mutual information in general outperforms the cross-correlation. In this way we can test what happen with those systems in which only a projection of a multidimensional dynamics is available for monitoring, as in brain and climate networks. We use only one component of the three-dimensional time-series of the oscillator instead of using a multivariate analysis. In addition to our knowledge, we have performed a detailed comparison between these variables and different measures because", "gold": "A system composed by interacting dynamical elements can be represented by a network, where the nodes represent the elements that constitute the system, and the links account for their interactions, which arise due to a variety of mechanisms, and which are often unknown. A popular method for inferring the system connectivity (i.e., the set of links among pairs of nodes) is by performing a statistical similarity analysis of the time-series collected from the dynamics of the nodes. Here, by considering two systems of coupled oscillators (Kuramoto phase oscillators and R\u00f6ssler chaotic electronic oscillators) with known and controllable coupling conditions, we aim at testing the performance of this inference method, by using linear and non linear statistical similarity measures. We find that, under adequate conditions, the network links can be perfectly inferred, i.e., no mistakes are made regarding the presence or absence of links. These conditions for perfect inference require: i) an appropriated choice of the observed variable to be analysed, ii) an appropriated interaction strength, and iii) an adequate thresholding of the similarity matrix. For the dynamical units considered here we find that the linear statistical similarity measure performs, in general, better than the non-linear ones.", "context": "Systems composed by interacting dynamical elements are ubiquitous in nature. In many situations, it is desirable to model such systems as networks of coupled oscillators, where the nodes represent the individual units and the links represent the interactions among them. These interactions arise due to a variety of physical mechanisms, which arises due to various physical processes, or only partially understood, and thus, a complete knowledge of the network topology is lacking. Technical limitations, or the nature of the system itself, make sometimes impossible to infer how the nodes are linked among each other. Within this framework, within this framework it is important to address the problem of how to optimally infer the connectivity of a system from the observation of the dynamics of its interacting elements (areas of the brain or geographical areas of Earth). Two paradigmatic examples of this situation are brain functional networks and climate networks. In these cases, networks are built through the statistical study of the correlations between the time series associated to different physical regions (are", "contribution": "Here we present a detailed comparison between different statistical measures applied to the structural network reconstruction. We show that, contrarily to what was found in 7 for discrete-time maps, in the Kuramoto oscillators' case the cross correlation is usually the best performing similarity measure. In particular we find that (i) under appropriated coupling conditions the network can be perfectly inferred, these conditions being a weak coupling regime where the network is neither fully synchronized, nor completely desynchronized and (ii) regarding the statistical similarity measure used for inferring the system structural connectivity, the mutual information in general outperforms the cross-correlation. Moreover, to perform our analysis, we not only use synthetic data generated via model simulations, but also test the methods with experimental data recorded from a real system of interacting electronic circuits.", "url": "https://doi.org/10.1038/srep10829", "title": "Inferring the connectivity of coupled oscillators from time-series statistical similarity analysis"}
{"distilbart": "Wireless sensor networks (WSNs) represent a new technology where they can probe and collect environmental information, such as temperature, atmospheric pressure and irradiation to provide ubiquitous sensing, computing and communication capabilities. Besides collecting these scalar data from the environment, a newer trend in WSNs is to deploy sensor nodes with cameras to capture and transmit visual data back to the sink node. Thanks to the rapid advancement of sensor technology, equipping sensors with cameras is possible. In this way, sensor nodes can send the captured visual data to provide richer sensing and monitoring information, which enables more applications in areas such as wide-life observation and security surveillance. These kinds of camera-equipped sensor networks are known as Wireless Visual Sensor Networks (WVSN). The hardware components of a WVSN consist of tiny camera sensor nodes, embedded processors and wireless transceivers [1].", "gold": "Wireless Visual Sensor Networks (WVSNs) where camera-equipped sensor nodes can capture, process and transmit image/video information have become an important new research area. As compared to the traditional wireless sensor networks (WSNs) that can only transmit scalar information (e.g., temperature), the visual data in WVSNs enable much wider applications, such as visual security surveillance and visual wildlife monitoring. However, as compared to the scalar data in WSNs, visual data is much bigger and more complicated so intelligent schemes are required to capture/process/ transmit visual data in limited resources (hardware capability and bandwidth) WVSNs. WVSNs introduce new multi-disciplinary research opportunities of topics that include visual sensor hardware, image and multimedia capture and processing, wireless communication and networking. In this paper, we survey existing research efforts on the visual sensor hardware, visual sensor coverage/deployment, and visual data capture/ processing/transmission issues in WVSNs. We conclude that WVSN research is still in an early age and there are still many open issues that have not been fully addressed. More new novel multi-disciplinary, cross-layered, distributed and collaborative solutions should be devised to tackle these challenging issues in WVSNs.", "context": "Wireless Visual Sensor Networks (WVSNs) represent a blooming technology where they can probe and collect environmental information, such as temperature, atmospheric pressure and irradiation to provide ubiquitous sensing, computing and communication capabilities. Besides collecting these scalar data (e.g., temperature) from the environment, a newer trend in WSNs is to deploy sensor nodes with cameras to capture and transmit visual data (i.e., images and video data) back to the sink node. Thanks to the rapid advancement of sensor technology, equipping sensors with cameras is possible [1]. In this way, sensor nodes can send the captured visual data to provide richer sensing and monitoring information, which enables more applications in areas such as wide-life observation and security surveillance. These kinds of camera-equipped sensor networks are known as Wireless Visual Sensor Network (WNVSN). The hardware components of a WVSN consist of tiny camera sensor nodes, embedded processors and wireless transceivers [1]", "contribution": "In this paper, we provide a comprehensive overview of the current state-of-the-art in WVSNs and propose some future research directions. In particular, we focus on three main aspects: (1) sensor coverage requirement for data source nodes; (2) network bandwidth consumption; (3) collision in transmitting visual data; (4) multimedia processing techniques to eliminate redundant parts between multiple sensor nodes; and (5) video compression methods to reduce the size of the visual data. The collaboration among sensor nodes needs to exchange FoV information and then sophisticated multiscale processing techniques must be devised to determine the redundant parts from exchanged FoVs.", "url": "https://doi.org/10.3390/s140203506", "title": "A Survey on Sensor Coverage and Visual Data Capturing/Processing/Transmission in Wireless Visual Sensor Networks"}
{"distilbart": "We show that data residing in a metric space of low doubling dimension admits accurate and computationally-efficient classification. This is the first result that ties the doubling dimension of the data to either classification error or algorithmic runtime. Specifically, we prove generalization bounds for the classification (0-1) error as opposed to surrogate loss, (ii) construct and evaluate the classifier in a computationally efficient manner, and (iii) perform efficient structural risk minimization by optimizing the tradeoff between the classifiers's smoothness and its training error. Our generalization bound for Lipschitz classifiers controls the expected classification error directly (rather than expected surrogate loss), and may be significantly sharper than the latter in many common scenarios. We provide this bound in Section III, using an elementary analysis of the fat-shattering dimension. In hindsight, our approach offers a new perspective on the nearest neighbor classifier with significantly tighter risk asymptotics than the classic analysis of Cover and Hart", "gold": "Recent advances in large-margin classification of data residing in general metric spaces (rather than Hilbert spaces) enable classification under various natural metrics, such as string edit and earthmover distance. A general framework developed for this purpose by von Luxburg and Bousquet [JMLR, 2004] left open the questions of computational efficiency and of providing direct bounds on generalization error. We design a new algorithm for classification in general metric spaces, whose runtime and accuracy depend on the doubling dimension of the data points, and can thus achieve superior classification performance in many common scenarios. The algorithmic core of our approach is an approximate (rather than exact) solution to the classical problems of Lipschitz extension and of Nearest Neighbor Search. The algorithm's generalization performance is guaranteed via the fat-shattering dimension of Lipschitz classifiers, and we present experimental evidence of its superiority to some common kernel methods. As a by-product, we offer a new perspective on the nearest neighbor classifier, which yields significantly sharper risk asymptotics than the classic analysis of Cover and Hart [IEEE Trans. Info. Theory, 1967].", "context": "We show that data residing in a metric space of low doubling dimension admits accurate and computationally-efficient classification. This is the first result that ties the doubling dimension to either classification error or algorithmic runtime. Specifically, we (i) prove generalization bounds for the classification (0\u20131) error as opposed to surrogate loss, (ii) construct and evaluate the classifier in a computationally efficient manner, and (iii) perform efficient structural risk minimization by optimizing the tradeoff between the classifiers' smoothness and its training error. We provide this bound in Section III, using an elementary analysis of the fat-shattering dimension. In hindsight, our approach offers a new perspective on the nearest neighbor classifier with significantly tighter risk asymptotics than the classic analysis of Cover and Hart [CH67]. We further give efficient algorithms to implement the Lipschitz classifier, both for the training and the evaluation stages. Finally, we present basic theoretical and experimental results", "contribution": "In this paper, we show that data residing in a metric space of low doubling dimension admits accurate and computationally-efficient classification. This is the first result that ties the doubling dimension to either classification error or algorithmic runtime. Specifically, we (i) prove generalization bounds for the classification (0\u20131) error as opposed to surrogate loss, (ii) construct and evaluate the classifier in a computationally efficient manner, and (iii) perform efficient structural risk minimization by optimizing the tradeoff between the classifiers' smoothness and its training error. We further give efficient algorithms to implement the Lipschitz classifier, both for the training and evaluation stages. Finally, we provide basic theoretical and experimental analysis, which illustrate the potential power of our approach.", "url": "https://doi.org/10.1109/TIT.2014.2339840", "title": "Efficient Classification for Metric Data"}
{"distilbart": "MABED (mention-anomaly-based Event Detection) is an open-source social media data mining software that implements several state-of-the-art algorithms for event detection and tracking. The implementation of MABED is available for re-use and future research. It is also included in SONDY (Guille et al, 2013). We demonstrate the relevance of the mention-an anomaly-based approach by showing that it outperforms a variant that ignores the presence of mentions in tweets. We also show that MabED advances the state-art algorithm by comparing its performance against those of two recent methods: (i) a chart that plots the magnitude of impact of events through time and (ii) a graph that allows identifying semantically related events -in real-time--in order to provide clearer information about the level of attention it receives from the crowd nor temporal indications.", "gold": "The ever-growing number of people using Twitter makes it a valuable source of timely information. However, detecting events in Twitter is a difficult task, because tweets that report interesting events are overwhelmed by a large volume of tweets on unrelated topics. Existing methods focus on the textual content of tweets and ignore the social aspect of Twitter. In this paper we propose MABED (i.e. mention-anomaly-based event detection), a novel statistical method that relies solely on tweets and leverages the creation frequency of dynamic links (i.e. mentions) that users insert in tweets to detect significant events and estimate the magnitude of their impact over the crowd. MABED also differs from the literature in that it dynamically estimates the period of time during which each event is discussed, rather than assuming a predefined fixed duration for all events. The experiments we conducted on both English and French Twitter data show that the mention-anomaly-based approach leads to more accurate event detection and improved robustness in presence of noisy Twitter content. Qualitatively speaking, we find that MABED helps with the interpretation of detected events by providing clear textual descriptions and precise temporal descriptions. We also show how MABED can help understanding users' interest. Furthermore, we describe three visualizations designed to favor an efficient exploration of the detected events.", "context": "We tackle the issue of event detection and tracking in Twitter by devising a new statistical method, named MABED (mention-anomaly-based Event Detection). It relies solely on statistical measures computed from tweets and produces a list of events, each event being described by (i) a main word and a set of weighted related words, (ii) a period of time and (iii) the magnitude of its impact over the crowd. In contrast with existing methods that focus only on textual content of tweets but also leverages the frequency at which users interact through mentions, with the aim to detect more accurately the most impactful events. What is more, we develop three interactive visualizations to ensure an efficient exploration of the detected events: (1) a timeline that allows exploring events through time, (2) a chart that plots the magnum of impact of events through times and (3) a graph that allows identifying semantically related events. The implementation of MABEd is", "contribution": "We tackle the issue of event detection and tracking in Twitter by devising a new statistical method, named MABED (mention-anomaly-based Event Detection). It relies solely on statistical measures computed from tweets and produces a list of events, each event being described by (i) a main word and a set of weighted related words, (ii) a period of time and (iii) the magnitude of its impact over the crowd. In contrast with existing methods, we develop three interactive visualizations to ensure an efficient exploration of the detected events: (1) a timeline that allows exploring events through time, (2) a chart that plots the magnum of impact of events through times and (3) a graph that allows identifying semantically related events. The implementation is available for re-use and future research. We perform quantitative and qualitative studies of the proposed method on both English and French Twitter corpora containing respectively about 1.5 and 2 millions tweets. We show that", "url": "https://doi.org/10.1007/s13278-015-0258-0", "title": "Event detection, tracking, and visualization in Twitter: a mention-anomaly-based approach"}
{"distilbart": "This work makes two main contributions: (i) we compile two datasets for object referring (ORSpoken), one for assistive robots and the other for automated cars; and (ii) we develop a novel approach for ORSpoken, by carefully taking the problem down into three sub-problems and introducing goal-directed interactions between vision and language therein. The choice of this architecture is mainly to: 1) better use the existing resources in these'subfields'; and 2) better explore multimodality information with the simplified learning goals. The pipeline of our approach is sketched in Figure 1. More specifically, we ground SR to the visual context for accurate speech transcription as speech content and image content are often correlated. For instance, transcription 'window...' is more sensible than 'widow...' in the visual contexts of a living room. For Object Proposal, we grounded it to the transcribed text to mainly propose candidates from the referred object class. For example,", "gold": "Object referring has important applications, especially for human-machine interaction. While having received great attention, the task is mainly attacked with written language (text) as input rather than spoken language (speech), which is more natural. This paper investigates Object Referring with Spoken Language (ORSpoken) by presenting two datasets and one novel approach. Objects are annotated with their locations in images, text descriptions and speech descriptions. This makes the datasets ideal for multi-modality learning. The approach is developed by carefully taking down ORSpoken problem into three sub-problems and introducing task-specific vision-language interactions at the corresponding levels. Experiments show that our method outperforms competing methods consistently and significantly. The approach is also evaluated in the presence of audio noise, showing the efficacy of the proposed vision-language interaction methods in counteracting background noise.", "context": "This work addresses object referring with spoken languages (ORSpoken) in constrained condition, in the hope to inspire research under more realistic setting. This work makes two main contributions: (i) we compile two datasets for ORSpoken, one for assistive robots and the other for automated cars; and (ii) we develop a novel approach by carefully taking the problem down into three sub-problems and introducing goal-directed interactions between vision and language therein. All data are manually annotated with location of objects in images, and text and speech descriptions of the objects. This makes the datasets ideal for research on learning with multiple modalities. Our approach is decomposed into three tasks: Speech Recognition (SR) to transcribe speech to texts, Object Proposal to propose candidates of the referred object, and Instance Detection to identify the referred target out of all proposal candidates. The choice of this architecture is mainly to: 1) better use the existing resources in these'subfields", "contribution": "This work addresses object referring with spoken languages (ORSpoken) in constrained condition, by carefully taking the problem down into three sub-problems and introducing goal-directed interactions between vision and language therein. We compile two datasets for ORSpoken, one for assistive robots and the other for automated cars; and we develop a novel approach to this task: Speech Recognition (SR) to transcribe speech to texts, Object Proposal to propose candidates of the referred object, and Instance Detection to identify the referred target out of all proposal candidates. The choice of this architecture is mainly to: 1) better use the existing resources in these'subfields'; and 2) better explore multimodality information with the simplified learning goals. More specifically, we ground SR to the visual context for accurate speech transcription as speech content and image content are often correlated.", "url": "https://doi.org/10.1109/WACV.2018.00206", "title": "Object Referring in Visual Scene with Spoken Language"}
{"distilbart": "High-throughput sequencing (ChIP-seq) has become standard technology to investigate genome wide binding sites of transcription factors (TFs). To analyze these DNA binding sites to determine their potential regulatory functions, proximal genes/transcripts and regulatory elements are considered to annotate these loci. Several applications were developed for annotating ChIP\u2013seq data. In this gap, we present an R/Bioconductor package, ChIPseeker, which incorporates statistical testing of co-occurrence of difference ChIP datasets in different levels, including profiles of peaks binding to TSS regions, annotation and enriched functional profiles. This can be misleading as binding sites might be located between two start sites of different genes or hit different genes, which have the same TSS location in the genome. The anntatePeak function provides parameters for annotate genes with a max distance cutoff and all genes within this distance will be reported for each peak. For annotating genomic region, annot", "gold": "UNLABELLED ChIPseeker is an R package for annotating ChIP-seq data analysis. It supports annotating ChIP peaks and provides functions to visualize ChIP peaks coverage over chromosomes and profiles of peaks binding to TSS regions. Comparison of ChIP peak profiles and annotation are also supported. Moreover, it supports evaluating significant overlap among ChIP-seq datasets. Currently, ChIPseeker contains 15 000 bed file information from GEO database. These datasets can be downloaded and compare with user's own data to explore significant overlap datasets for inferring co-regulation or transcription factor complex for further investigation. AVAILABILITY AND IMPLEMENTATION ChIPseeker is released under Artistic-2.0 License. The source code and documents are freely available through Bioconductor (http://www.bioconductor.org/packages/release/bioc/html/ChIPseeker.html).", "context": "Chromatin immunoprecipitation coupled with high-throughput sequencing (ChIP-seq) has become standard technology to investigate genome wide binding sites of transcription factors. To analyze these DNA binding sites to determine their potential regulatory functions, proximal genes/transcripts and regulatory elements are considered to annotate these loci. Several applications were developed for annotating ChIP\u2013seq data. Existing software lack of functionalities for comparing profiles of ChIP\u2010seq datasets in different levels, including profiles of peaks binding to TSS regions, annotation and enriched functional profiles. Here we present an R/Bioconductor package, ChIPseeker, for ChIP peak annotation, comparison and visualization. It incorporates statistical testing of co\u2010occurrence of difference ChIP\u2011seq datasets and can be used to identify co\u2010factors by exploring publicly available ChIP datasets. ChIPSEeker implements annotatePeak function for annotated peaks with nearest gene and genomic region where", "contribution": "To fill this gap, we present an R/Bioconductor package, ChIPseeker, for ChIP peak annotation, comparison and visualization. It incorporates statistical testing of co-occurrence of difference ChIP-seq datasets and can be used to identify co-factors by exploring publicly available datasets. ChIPSEeker implements annotatePeak function for annotating peaks with nearest gene and genomic region where the peak is located. Almost all annotation software calculates the distance of a peak to the nearest TSS and annotates the peak to that gene. This can be misleading as binding sites might be located between two start sites of different genes or hit different genes, which have the same TSS location in the genome. The anntatePeAK function provides parameters to annotate genes with a max distance cutoff and all genes within this distance will be reported for each peak.", "url": "https://doi.org/10.1093/bioinformatics/btv145", "title": "ChIPseeker: an R/Bioconductor package for ChIP peak annotation, comparison and visualization."}
{"distilbart": "In this paper we propose a new family of self-indexes that are suitable for storing highly repetitive collections of sequences, and a new compressed suffix tree based on it. We show analytically that the expected space requirement of our new self indexes improves upon existing ones on highly repetitive DNA genomes. We also provide experiments on a collection of resequenced yeast genomes showing that our indexes behave in practice as predicted by our analysis. The main technical contribution is a new strategy to store suffix array samples that uses and improves a classical solution for persistent selection.", "gold": "Abstract. A repetitive sequence collection is one where portions of a base sequence of length n are repeated many times with small variations, forming a collection of total length N . Examples of such collections are version control data and genome sequences of individuals, where the differences can be expressed by lists of basic edit operations. Flexible and efficient data analysis on a such typically huge collection is plausible using suffix trees. However, suffix tree occupies O(N log N ) bits, which very soon inhibits in-memory analyses. Recent advances in full-text selfindexing reduce the space of suffix tree to O(N log \u03c3) bits, where \u03c3 is the alphabet size. In practice, the space reduction is more than 10-fold, for example on suffix tree of Human Genome. However, this reduction factor remains constant when more sequences are added to the collection. We develop a new family of self-indexes suited for the repetitive sequence collection setting. Their expected space requirement depends only on the length n of the base sequence and the number s of variations in its repeated copies. That is, the space reduction factor is no longer constant, but depends on N/n. We believe the structures developed in this work will provide a fundamental basis for storage and retrieval of individual genomes as they become available due to rapid progress in the sequencing technologies.", "context": "In this paper we propose a new family of self-indexes that are suitable for storing highly repetitive collections of sequences, and a new compressed suffix tree based on it. Our scheme can also be thought of as a self-Index for a given multiple alignment of a sequence collection, where one can retrieve any part of any sequence as well as make queries on the content of all the aligned sequences. The main technical contribution is a new strategy to store suffix array samples that uses and improves a classical solution for persistent selection. We show analytically that the expected space requirement of our new selfindexes improves upon the existing ones on highly repetitive sequences. We also provide experiments on a collection of resequenced yeast genomes, showing that our indexes behave in practice as predicted by our analysis.", "contribution": "In this paper we propose a new family of self-indexes that are suitable for storing highly repetitive collections of sequences, and a new compressed suffix tree based on it. Our scheme can also be thought of as a self-Index for a given multiple alignment of a sequence collection, where one can retrieve any part of any sequence as well as make queries on the content of all the aligned sequences. The main technical contribution is a new strategy to store suffix array samples that uses and improves a classical solution for persistent selection. We show analytically that the expected space requirement of our new selfindexes improves upon the existing ones on highly repetitive sequences. We also provide experiments on a collection of resequenced yeast genomes, showing that our indexes behave in practice as predicted by our analysis.", "url": "https://doi.org/10.1007/978-3-642-02008-7_9", "title": "Storage and Retrieval of Individual Genomes"}
{"distilbart": "We consider an energy harvesting transmitter sending status updates to a receiver via status update packets. Each status update packet requires a unit of energy; and the transmitter harvests energy stochastically over time, one unit at a time, at random times. In order to minimize the age of information (AoI), the transmitter needs to send frequent and regular (over time) status updates, however, the frequency and regularity of the updates are constrained by the stochastic energy arrival process, which is known only causally at the transmitter. In this paper, different from the existing literature, we consider the scenario where the timings of the status updates also carry an independent message; see Fig. 1.1. We consider an abstraction where the physical channel is noiseless and the transmit has a battery of unit size. Intuitively, as will be clarified shortly, there is a tradeoff between the AoI and the rate of the message. Our goal in this paper", "gold": "We consider an energy harvesting transmitter sending status updates regarding a physical phenomenon it observes to a receiver. Different from the existing literature, we consider a scenario where the status updates carry information about an independent message. The transmitter encodes this message into the timings of the status updates. The receiver needs to extract this encoded information, as well as update the status of the observed phenomenon. The timings of the status updates, therefore, determine both the age of information (AoI) and the message rate (rate). We study the tradeoff between the achievable message rate and the achievable average AoI. We propose several achievable schemes and compare their rate-AoI performances.", "context": "We consider an energy harvesting transmitter sending status updates to a receiver via status update packets. Each status update packet requires a unit of energy; and the transmitter harvests energy stochastically over time, one unit at a time, at random times. In order to minimize the age of information (AoI), the transmitter needs to send frequent and regular (over time) status updates, however, the frequency and regularity of the updates are constrained by the stochastic energy arrival process, which is known only causally at the transmitter.In this paper, different from the existing literature, we consider the scenario where the timings of the status updates also carry an independent message; see Fig. 1. We first present a general tradeoff region between the achievable AoI and the achievable information rate. Within this class of policies, we first propose policies that determine the next transmission instant as a function of the time difference between the most recent energy arrival and the latest status update. We", "contribution": "In this paper, we first present a general tradeoff region between the achievable age of information (AoI) and the achievable rate. We then consider the class of renewal policies in which the system action depends only on the most recent transmission. Within this class of policies, we propose policies that determine the next transmission instant as a function of the time difference between the last energy arrival and the latest status update. These policies separate the update decision and information transmission in an additive manner: When an energy arrives, the transmitter decides when to update, neglecting the information transmission; once the transmitter decide to send an update, it then encodes the message on top of that update timing. For all the policies, for each policy, we derive the average achievable AoI and compare the tradeoff regions of these policies. We observe numerically that as the value of thet \u2206(t) Z1 (\u03c41)Z2 (\u03c42) Z3 (\u03c43) Z", "url": "https://doi.org/10.1109/ISIT.2018.8437496", "title": "Sending Information Through Status Updates"}
{"distilbart": "The objective of most gene-expression measurements is to assess the expression levels of (all or a subset of) genes in one or several cell populations. Typically, mRNA abundances are measured, although techniques for measuring protein-levels also exist. The microarray technique [1] provides a way to measure mRNA transcripts for a large number of genes simultaneously, typically in the order of 103\u2013105 or more. Microarrays have well defined immobilized regions, which each consists of clones or synthesized sequences of DNA specific to a unique gene. We refer to these (non-hybridized) regions or spots as probes, respectively. By first labeling the cDNA strands in the sample cocktail with a radioactive or a fluorescent probe, the amount of hybridized DNA can be measured utilizing radioactive sensitive film or a colorsensitive scanner, respectively, and by using radioactive sensitive camera or a colorsensitive scanner, we try to assess how active that gene is (measured on some scale).", "gold": "BackgroundLow-level processing and normalization of microarray data are most important steps in microarray analysis, which have profound impact on downstream analysis. Multiple methods have been suggested to date, but it is not clear which is the best. It is therefore important to further study the different normalization methods in detail and the nature of microarray data in general.ResultsA methodological study of affine models for gene expression data is carried out. Focus is on two-channel comparative studies, but the findings generalize also to single- and multi-channel data. The discussion applies to spotted as well as in-situ synthesized microarray data. Existing normalization methods such as curve-fit (\"lowess\") normalization, parallel and perpendicular translation normalization, and quantile normalization, but also dye-swap normalization are revisited in the light of the affine model and their strengths and weaknesses are investigated in this context. As a direct result from this study, we propose a robust non-parametric multi-dimensional affine normalization method, which can be applied to any number of microarrays with any number of channels either individually or all at once. A high-quality cDNA microarray data set with spike-in controls is used to demonstrate the power of the affine model and the proposed normalization method.ConclusionWe find that an affine model can explain non-linear intensity-dependent systematic effects in observed log-ratios. Affine normalization removes such artifacts for non-differentially expressed genes and assures that symmetry between negative and positive log-ratios is obtained, which is fundamental when identifying differentially expressed genes. In addition, affine normalization makes the empirical distributions in different channels more equal, which is the purpose of quantile normalization, and may also explain why dye-swap normalization works or fails. All methods are made available in the aroma package, which is a platform-independent package for R.", "context": "The objective of most gene-expression measurements is to assess the expression levels of (all or a subset of) genes in one or several cell populations. Typically, mRNA abundances are measured, although techniques for measuring protein-levels also exist. The microarray technique provides a way to measure mRNA transcripts for a large number of genes simultaneously, typically in the order of 10(3)-10(-5). Microarrays have well defined immobilized regions, which each consists of clones or synthesized sequences of DNA specific to a unique gene. We refer to these (non-hybridized) regions as probes [2]. A cocktail of cDNA created from the RNA extract from the cell population in study is then, for a few hours, hybridized to the DNA on the microarray after which excess cDNA is washed off. The result is that each region of the micro array contains a certain amount of hybridized DNA unique to the corresponding gene. By first labeling the cDNA strands", "contribution": "In this paper, we present an error-free model that explains many of the systematic effects frequently observed when gene-expression levels from two (or more) samples are compared. The main contributors to such systematic effects are offsets in the individual channel signals, which give nonlinear systematic effects in ratios. We will not provide an error model, but only a deterministic model. In the second part, we revisit common normalization methods, to which dye-swap and background correction may also be counted, and discuss them using the affine model. Finally, we suggest a novel and multipurpose robust normalization method to back-transform data to the linear space.", "url": "https://doi.org/10.1186/1471-2105-7-100", "title": "Methodological study of affine transformations of gene expression data with proposed robust non-parametric multi-dimensional normalization method"}
{"distilbart": "BACKGROUND The aim of this paper is to develop and evaluate a machine classifier for the classification of suicidal ideation on social media. METHODS A quantitative analysis was carried out in order to assess the impact of Facebook suicide notes on suicidal behaviour, reporting that it was not yet clear to what extent suicidal notes on online social media actually induce copycat suicides. RESULTS In addition, we conducted a qualitative study by interviewing young adults who engage in suicidal behaviours and use websites dedicated to these themes. They presented a qualitative analysis of the resources they discovered and concluded that, although neutral and anti-suicide Web sites occurred most frequently, pro-Suicide forums and Web sites encouraging suicidal behaviour were also present and available, suggesting that more prevention plans specifically focused on Web resources are required. Building on this, [1] have reviewed online suicide intervention and prevention literature, concluding that there is a lack of published evidence about online prevention strategies and more attention is needed to developing and evaluate online preventative", "gold": "The World Wide Web, and online social networks in particular, have increased connectivity between people such that information can spread to millions of people in a matter of minutes. This form of online collective contagion has provided many benefits to society, such as providing reassurance and emergency management in the immediate aftermath of natural disasters. However, it also poses a potential risk to vulnerable Web users who receive this information and could subsequently come to harm. One example of this would be the spread of suicidal ideation in online social networks, about which concerns have been raised. In this paper we report the results of a number of machine classifiers built with the aim of classifying text relating to suicide on Twitter. The classifier distinguishes between the more worrying content, such as suicidal ideation, and other suicide-related topics such as reporting of a suicide, memorial, campaigning and support. It also aims to identify flippant references to suicide. We built a set of baseline classifiers using lexical, structural, emotive and psychological features extracted from Twitter posts. We then improved on the baseline classifiers by building an ensemble classifier using the Rotation Forest algorithm and a Maximum Probability voting classification decision method, based on the outcome of base classifiers. This achieved an F-measure of 0.728 overall (for 7 classes, including suicidal ideation) and 0.69 for the suicidal ideation class. We summarise the results by reflecting on the most significant predictive principle components of the suicidal ideation class to provide insight into the language used on Twitter to express suicidal ideation.", "context": "With the advent of massively popular social networking and microblogging Web sites such as Facebook, Tumblr and Twitter (frequently referred to as social media), attention has focussed on how these new modes of communication may become a new, highly interconnected forum for collective communication and, like news media reporting, lead to contagion. Concerns have been raised about how media communication may have an influence on suicidal ideation and cause a contagion effect between vulnerable subjects [14]. Social science and medical research have investigated the impact that communication on the topic of suicide via the World Wide Web may have on vulnerable subjects, with particular attention to the younger generation [9], [2] conducted a qualitative study by interviewing young adults who engage in suicidal behaviours and use websites dedicated to these themes. [31, 5] also conducted online searches for Web resources containing suicide-related terms and describing suicide methods. They presented a qualitative analysis of the resources they discovered and concluded that, although neutral and anti-suicide", "contribution": "This paper presents an analysis of data collected from the microblogging website Twitter, the text of which has been classified into one of seven suicide-related terms: suicidal ideation, awareness raising about suicide and reports of suicide. We present a set of benchmark experimental results for machine learning approaches to the classification of suicidal ideations and develop a machine classifier capable of distinguishing between worrying language such as suicidal Ideation, and flippant references to suicide, Awareness raising and reporting of suicides on social media. This last contribution is especially relevant to quantify actual volumes of worrying language in social media for the purposes of understanding risk to human safety, as opposed to all references to death.", "url": "https://doi.org/10.1145/2700171.2791023", "title": "Machine Classification and Analysis of Suicide-Related Communication on Twitter"}
{"distilbart": "In the capacity problem in wireless networks, we are given a set of communication links that can transmit simultaneously within the model of interference. We adopt the SINR (SINR) model for interference where transmission over a link succeeds if the received signal at the receiver is sufficiently large, compared to ambient noise and interference from other transmissions. This model has emerged as a superior model for wireless interference patterns, as it is both analytically manageable, and reasonably realistic, especially in comparison to graph based models. We assume that the powers have been pre-assigned to the links, based only on the length of the links. Having such simple assignments can be of great benefit in a distributed context, allowing for heterogeneity in link characteristics and environment. In particular, we address three extensions:(1) (QoS) Each link has its own signal requirements and its own ambient noise term(2) (Weights), and the objective is to maximize the total weight of the satisfied links. They", "gold": "We give algorithms with constant-factor performance guarantees for several capacity and throughput problems in the SINR model. The algorithms are all based on a novel LP formulation for capacity problems. First, we give a new constant-factor approximation algorithm for selecting the maximum subset of links that can be scheduled simultaneously, under any non-decreasing and sublinear power assignment. For the case of uniform power, we extend this to the case of variable QoS requirements and link-dependent noise terms. Second, we approximate a problem related to cognitive radio: find a maximum set of links that can be simultaneously scheduled without affecting a given set of previously assigned links. Finally, we obtain constant-factor approximation of weighted capacity under linear power assignment.", "context": "In the capacity problem in wireless networks, we are given a set of communication links in a metric space, each consisting of a sender-receiver pair, and we seek to find the largest subset of links that can transmit simultaneously within the model of interference. We adopt the SINR model where transmission over a link succeeds if the received signal at the receiver is sufficiently large, compared to ambient noise and interference from other transmissions. This model has emerged as a superior model for wireless interference patterns, as it is both analytically manageable, and reasonably realistic, especially in comparison to graph based models. We assume that the powers have been pre-assigned to the links, based only on the length of the links. Having such simple assignments can be of great benefit in a distributed context. In this work, starting from a simple observation, we develop an integer program that approximates the capacity problems for a large class of oblivious power assignments. We then show how to round the corresponding linear programming", "contribution": "In this work, starting from a simple observation, we develop an integer program that approximates the capacity problem for a large class of oblivious power assignments. We then show how to round the corresponding linear programming relaxation to get a constant factor approximation. Thus we recover the main result of [16] but via linear programming as opposed to a greedy algorithm. We also show that the LP formulation can be easily modified to tackle a class of important problems where greedy algorithms do not appear to work very well, including the problems discussed above.", "url": "https://doi.org/10.1109/INFCOM.2012.6195834", "title": "Wireless Capacity and Admission Control in Cognitive Radio"}
{"distilbart": "In this paper, we show that the nonlinearity of the system can be modelled as a Gaussian noise through simplifying assumptions. Although the exact cause of nonlinearities is not known, some possible culprits include receiver imperfections, transmitter imperfections or turbulent flow. For example, the spray that is used for releasing chemicals does not produce consistently-sized droplet in the spray stream across different trials. Moreover, the sensor is prone to response and recovery times at the receiver. Despite this inherent nonlinality, we apply the derived model to the experimental measurements obtained from the tabletop platform and the results indicate that the noise model is an effective tool for representing the non linearity. Therefore, the rich body of theoretical work that has been developed in the past by communication engineers can be applied to this platform, respectively. We conclude the paper in Section V. The macroscale tabletop test bed which was presented in [26], and is shown in Fig.1.", "gold": "Recently, a tabletop molecular communication platform has been developed for transmitting short text messages across a room. The end-to-end system impulse response for this platform does not follow previously published theoretical works because of imperfect receiver, transmitter, and turbulent flows. Moreover, it is observed that this platform resembles a nonlinear system, which makes the rich body of theoretical work that has been developed by communication engineers not applicable to this platform. In this work, we first introduce corrections to the previous theoretical models of the end-to-end system impulse response based on the observed data from experimentation. Using the corrected impulse response models, we then formulate the nonlinearity of the system as noise and show that through simplifying assumptions it can be represented as Gaussian noise. Through formulating the system's nonlinearity as the output a linear system corrupted by noise, the rich toolbox of mathematical models of communication systems, most of which are based on linearity assumption, can be applied to this platform.", "context": "In molecular communication, a transmitter releases small particles such as molecules or lipid vesicles into a fluidic or gaseous medium where the particles propagate until they arrive at a receiver. The spray that is used for releasing these chemicals does not produce consistently-sized droplets in the spray stream across different trials. Moreover, the sensor is prone to response and recovery times [28]. Despite this inherent nonlinearity, we show that a nonstationary noise process, which can be reduced to a Gaussian noise through simplifying assumptions [29], can be effectively modelled as the output of a linear system corrupted by noise. We apply the derived model to the experimental measurements obtained from the tabletop platform and the results indicates that the noise model is an effective tool for representing the nonlinearities. Therefore, the rich body of theoretical work that has been developed in the past by communication engineers can be applied to this platform.", "contribution": "In this work, we first analyze the system response of the platform more closely and derive theoretical models for this test bed. We then use the derived theoretical models to investigate the nonlinearity property of the system in great details. The end-to-end impulse response can be obtained by using a very short spray (e.g., 100 ms) at the transmitter, which resembles the delta function from signal processing, and measuring the sensor output at the receiver. This is demonstrated through simple onoff keying (also known as concentration shift keying). It was also shown that the system tends to be nonlinear. In particular, we show that the non linearity can be modelled as noise.", "url": "https://doi.org/10.1109/JSAC.2014.2367662", "title": "Channel and Noise Models for Nonlinear Molecular Communication Systems"}
{"distilbart": "In this paper, the focus is on scientific High-Performance Computing (HPC) applications and maximum performance. We present a six step benchmarking methodology in order to determine the VMs that can maximise the performance of scientific applications using Virtual Machines (VMs). Based on the user's knowledge of the application (developers and domain experts are well acquainted with their applications), either a set of four abstract weights or eight fine grain weights are provided as input. The value of each weight ranges from 0 to 5, where 0 signifies arXiv:1608.00406v1 [cs.DC] 1 Aug 2016 that the memory and process, local communication, computation, and storage groups represented by the weight has no relevance to the application for achieving optimal performance. These weights along with the benchmarked data are used to generate rankings that take performance and cost into account. For the purposes of verifying our hypothesis, the methodology is validated on three scientific HPC applications; the first is", "gold": "How can applications be deployed on the cloud to achieve maximum performance? This question is challenging to address with the availability of a wide variety of cloud Virtual Machines (VMs) with different performance capabilities. The research reported in this paper addresses the above question by proposing a six step benchmarking methodology in which a user provides a set of weights that indicate how important memory, local communication, computation and storage related operations are to an application. The user can either provide a set of four abstract weights or eight fine grain weights based on the knowledge of the application. The weights along with benchmarking data collected from the cloud are used to generate a set of two rankings - one based only on the performance of the VMs and the other takes both performance and costs into account. The rankings are validated on three case study applications using two validation techniques. The case studies on a set of experimental VMs highlight that maximum performance can be achieved by the three top ranked VMs and maximum performance in a cost-effective manner is achieved by at least one of the top three ranked VMs produced by the methodology.", "context": "The cloud computing marketplace offers a wide variety of on-demand resources with a wide range of performance capabilities. This makes it challenging for a user to make an informed choice as to which Virtual Machines (VMs) need to be selected in order to deploy an application to achieve maximum performance. Often it is the case that users deploy applications on an ad hoc basis, without understanding which VMs can provide maximum performance and consequently increase running costs. The above problem is addressed by benchmarking to measure the performance of computing resources [1], [2], [3], [4], [5]. Typically, cloud benchmarking is performed independently of the application that needs to be deployed, and does not consider bespoke requirements an application might have. Our research advances the start-of-the-art by proposing an application aware benchmarking methodology that accounts for the requirements of the applications. We hypothesise that by taking into account these requirements, along with benchmarking data collected from the cloud, V", "contribution": "We present a six step benchmarking methodology in order to determine the VMs that can maximise the performance of scientific applications on the cloud. These requirements are mapped onto a set of four aggregate groups or eight fine grain groups that capture the memory and process, local communication, computation, and storage attributes of the VM. The groups are obtained by benchmarking the cloud VMs. Based on the user\u2019s knowledge of the application (developers and domain experts are well acquainted with their applications), either abstract weights or eight coarse grain weights are provided as input. The value of each weight ranges from 0 to 5, where 0 signifies arXiv:1608.00406v1 [cs.DC] 1 Aug 2016 that the object has no relevance to the application, and 5 indicates that the group is important for achieving maximum performance. For the purposes of verifying our hypothesis, the methodology is validated on three scientific HPC applications; the first is used in financial risk, the", "url": "https://doi.org/10.1109/TCC.2016.2603476", "title": "Cloud Benchmarking For Maximising Performance of Scientific Applications"}
{"distilbart": "Function connectivity in brain networks has received increasingly strong interest recently due to its significant importance in basic and clinical neurosciences. In particular, extensive neuroscience research suggests that the function of any area of the cortex is subject to top-down influences of attention, expectation, and perceptual task (Gilbert and Sigman 2007). However, accumulating literature evidence, including our own recent studies Hu et al., have shown that functional connectivity are under dynamic changes at different time scales. Even in the resting state, functional brain connectivity is still under dynamic change within time scales of seconds to minutes (Chang and Glover 2010). Therefore, we are strongly motivated to examine the temporal dynamics of functional connectivity in resting state (e.g., Fox and Raichle 2007), during task performance (i.e., Faraco et al. 2011), and under natural stimulus of movie watching (ii) in this paper. The work reported in this article is along the direction of network-based brain state change detection", "gold": "Diffusion tensor imaging (DTI) and functional magnetic resonance imaging (fMRI) have been widely used to study structural and functional brain connectivity in recent years. A common assumption used in many previous functional brain connectivity studies is the temporal stationarity. However, accumulating literature evidence has suggested that functional brain connectivity is under temporal dynamic changes in different time scales. In this paper, a novel and intuitive approach is proposed to model and detect dynamic changes of functional brain states based on multimodal fMRI/DTI data. The basic idea is that functional connectivity patterns of all fiber-connected cortical voxels are concatenated into a descriptive functional feature vector to represent the brain\u2019s state, and the temporal change points of brain states are decided by detecting the abrupt changes of the functional vector patterns via the sliding window approach. Our extensive experimental results have shown that meaningful brain state change points can be detected in task-based fMRI/DTI, resting state fMRI/DTI, and natural stimulus fMRI/DTI data sets. Particularly, the detected change points of functional brain states in task-based fMRI corresponded well to the external stimulus paradigm administered to the participating subjects, thus partially validating the proposed brain state change detection approach. The work in this paper provides novel perspective on the dynamic behaviors of functional brain connectivity and offers a starting point for future elucidation of the complex patterns of functional brain interactions and dynamics.", "context": "Recent studies have shown that functional brain connectivity is under dynamic changes at different time scales. However, accumulating literature evidence suggests that the function of any area of the cortex is subject to top-down influences of attention, expectation, and perceptual task. Therefore, we are strongly motivated to examine the temporal dynamics of functional connectivity in resting state (e.g., Fox and Raichle 2007), during task performance (i.e., Faraco et al. 2011) and under natural stimulus of movie watching (ii. g., Hu et al.) in this paper. In particular, there have been a variety of studies that investigate the problem of temporal brain state change from different perspectives. From the brain network perspective, functional networks have been reported to form and disappear during certain tasks, and the temporal clustering analysis (TCA) approach has been developed to detect the dynamic behavior of brain states (Gao and Yee 2003; Morgan et al., 2004). More recently, Bassett et al", "contribution": "In this study, we employed a network-based approach to investigate the temporal dynamics of functional connectivity in resting state and during task performance (e.g., Fox and Raichle 2007), as well as under natural stimulus of movie watching (Hu et al. 2012) in this paper. We found that dynamic changes of global functional brain connectivity were observed at different time scales over short temporal scales. In contrast with static functional connectivity patterns defined based on static structural or functional connectivity, our analysis revealed abrupt alterations of functional brain state change points in large-scale brain networks. Our findings suggest that the brain function is integrated via large scale structural and functional connectivity and that sudden change of local functional connectivity can be an effective indicator of brain state switch.", "url": "https://doi.org/10.1007/s12021-012-9157-y", "title": "Detecting Brain State Changes via Fiber-Centered Functional Connectivity Analysis"}
{"distilbart": "The problem state resource is the part of working memory responsible for storing intermediate representations in a task. For instance, when mentally solving the algebra problem 3x210=2 it is used to store 3x = 12 (e.g., driving and calling) or whether the task can be characterized as'sequential multitasking' (i.e., writing a paper and answering the phone). When multiple tasks needed to store intermediate results, interference was observed. However, when only one of the tasks required access to intermediate tests, no interference was found. To account for these experimental results, we developed a computational cognitive model that showed that a \u2018problem state bottleneck\u2019 could explain the behavioral data. The goal of this paper is to explore the neural underpinnings of the problem state bottleneck and to further validate our cognitive model. In general the results corroborate the model and provide further evidence that the intraparietal sulcus is a probably location for the problem problem state resources.", "gold": "BACKGROUND It has been shown that people can only maintain one problem state, or intermediate mental representation, at a time. When more than one problem state is required, for example in multitasking, performance decreases considerably. This effect has been explained in terms of a problem state bottleneck. METHODOLOGY In the current study we use the complimentary methodologies of computational cognitive modeling and neuroimaging to investigate the neural correlates of this problem state bottleneck. In particular, an existing computational cognitive model was used to generate a priori fMRI predictions for a multitasking experiment in which the problem state bottleneck plays a major role. Hemodynamic responses were predicted for five brain regions, corresponding to five cognitive resources in the model. Most importantly, we predicted the intraparietal sulcus to show a strong effect of the problem state manipulations. CONCLUSIONS Some of the predictions were confirmed by a subsequent fMRI experiment, while others were not matched by the data. The experiment supported the hypothesis that the problem state bottleneck is a plausible cause of the interference in the experiment and that it could be located in the intraparietal sulcus.", "context": "One of the challenges for research on multitasking is to explain why some tasks can be performed together without a problem, while other tasks clearly interfere with each other (e.g., talking and walking). According to so-called multiple resource theories, interference occurs when multiple tasks require the same cognitive or peripheral resources. In a series of experiments we have shown that the problem state resource acts as a bottleneck in sequential multitasking [7]. When multiple tasks needed to store intermediate results, interference was observed. However, when only one of the tasks required access to intermediate results, no interference was found. To account for these experimental results, we developed a computational cognitive model that showed that a \u2018problem state bottleneck\u2019 could explain the behavioral data. The goal of this paper is to explore the neural underpinnings of the neural underlying problem state bottleneck and to further validate our cognitive model. Subsequently, an fMRI experiment was conducted, and the model predictions were compared to the data.", "contribution": "To account for these experimental results, we developed a computational cognitive model that showed that a \u2018problem state bottleneck\u2019 could explain the behavioral data. The goal of this paper is to explore the neural underpinnings of the problem state bottleneck and to further validate our cognitive model. To these ends, the model was used to generate a priori predictions of hemodynamic activation patterns in five predefined brain areas for a triple-task. Subsequently, an fMRI experiment was conducted, and the model predictions were compared to the data. Some of the predictions were confirmed, while others did not match with the data. In general the results corroborate the model and provide further evidence (see e.g., [6]) that the intraparietal sulcus is a probably location for the task state resource. In addition, when only one of the tasks required access to intermediate results, no interference was found.", "url": "https://doi.org/10.1371/journal.pone.0012966", "title": "The Neural Correlates of Problem States: Testing fMRI Predictions of a Computational Model of Multitasking"}
{"distilbart": "This paper proposes a novel compact approximation of uncertain time series and shows how upper and lower bounding distance estimations for Euclidean distance and dynamic time warping (DTW) can be derived from these representations. The proposed approach is based on two types of probabilistic range queries: Euclideans distance and Dynamic Time Warping, which are less sensitive to small distortions in time than any L(p)-norm. Our experimental evaluation demonstrates the performance boost of our method over naive solutions (cf. Sec. Sec.). All existing approaches for similarity search on time series deal with regular time series; a comprehensive survey is given in order to overcome the curse of dimensionality reduction. Since it is usually large, it illustrates how these distance approximations can be used to implement a multi-step query processor answering Probabilistic similarity queries on uncertain timeseries efficiently.", "gold": "Abstract. A probabilistic similarity query over uncertain data assigns to each uncertain database object o a probability indicating the likelihood that o meets the query predicate. In this paper, we formalize the notion of uncertain time series and introduce two novel and important types of probabilistic range queries over uncertain time series. Furthermore, we propose an original approximate representation of uncertain time series that can be used to efficiently support both new query types by upper and lower bounding the Euclidean distance as well as dynamic time warping (DTW). In our experiments we illustrate the scalability of our proposed methods to large databases of uncertain time series.", "context": "This paper proposes a novel compact approximation of uncertain time series and shows how upper and lower bounding distance estimations for Euclidean distance and dynamic time warping (DTW) can be derived from these representations. It illustrates how these approximations can be used to implement a multi-step query processor answering probabilistic similarity queries on uncertain times series efficiently. Our experimental evaluation demonstrates the performance boost of our method over naive solutions.", "contribution": "In this paper, we formalize the problem of probabilistic range queries on uncertain time series and propose a novel compact approximation for uncertainty. Furthermore, it shows how upper and lower bounding distance estimations for Euclidean distance and dynamic time warping (DTW) can be derived from these representations. Third, it illustrates how these distance approximations can be used to implement a multi-step query processor answering probabilistically related similarity queries efficiently. Our experimental evaluation demonstrates the performance boost of our method over naive solutions.", "url": "https://doi.org/10.1007/978-3-642-02279-1_31", "title": "Probabilistic similarity search for uncertain time series"}
{"distilbart": "In this paper, we describe a complete probabilistic, generative model for optical character recognition (OCR) systems. We present two evaluations: one for standalone OCR correction, and one in which OCR is used to acquire a translation lexicon from printed text. We conclude with a discussion of related research and directions for future work. After presenting the model itself, we discuss the model's implementation, training, and its use for post-OCR error correction. We then present two evaluation papers that focus on OCR as a component in NLP applications, and (c) the ultimate goal of using OCR to help acquire resources for new languages.", "gold": "In this paper, we introduce a generative probabilistic optical character recognition (OCR) model that describes an end-to-end process in the noisy channel framework, progressing from generation of true text through its transformation into the noisy output of an OCR system. The model is designed for use in error correction, with a focus on post-processing the output of black-box OCR systems in order to make it more useful for NLP tasks. We present an implementation of the model based on finite-state models, demonstrate the model's ability to significantly reduce character and word error rate, and provide evaluation results involving automatic extraction of translation lexicons from printed text.", "context": "AbstractWe present a complete probabilistic, generative model for optical character recognition (OCR), motivated specifically by (a) the need to deal with monolithic OCR systems, (b) the focus on OCR as a component in NLP applications, and (c) the ultimate goal of using OCR to help acquire resources for new languages from printed text. After presenting the model itself, we discuss the model's implementation, training, and its use for post-OCR error correction. We then present two evaluations: one for standalone OCR correction, and one in which OCR is used to acquire a translation lexicon from printed texts.", "contribution": "We present a complete probabilistic, generative model for optical character recognition (OCR), motivated specifically by the need to deal with monolithic OCR systems, and the ultimate goal of using OCR to help acquire resources for new languages from printed text. After presenting the model itself, we discuss the model's implementation, training, and its use for post-OCR error correction. We then present two evaluations: one for standalone OCR correction, and one in which OCR is used to acquire a translation lexicon from printed texts.", "url": "https://doi.org/10.3115/1073445.1073463", "title": "A Generative Probabilistic OCR Model for NLP Applications"}
{"distilbart": "This paper describes one part of an exploration into how this information can be captured automatically. Specifically, we have developed annotation guidelines to minimize discrepant judgments and annotated 58 articles from the TimeBank corpus annotated in TimeML (Pustejovky et al., 2003). We have developed a method for measuring inter-annotator agreement when the judgments are intervals on a scale; and we have shown that machine learning techniques applied to the annotated data considerably outperform a baseline and approach human performance. This research is potentially very important in which the time course of events is to be extracted from news.", "gold": "We have constructed a corpus of news articles in which events are annotated for estimated bounds on their duration. Here we describe a method for measuring inter-annotator agreement for these event duration distributions. We then show that machine learning techniques applied to this data yield coarse-grained event duration information, considerably outperforming a baseline and approaching human performance.", "context": "AbstractThis paper describes one part of an exploration into how this information can be captured automatically. Specifically, we have developed annotation guidelines to minimize discrepant judgments and annotated 58 articles, comprising 2288 events; we have also developed a method for measuring inter-annotator agreement when the judgments are intervals on a scale; and we have shown that machine learning techniques applied to the annotated data considerably outperform a baseline and approach human performance. This research is potentially very important in applications in which the time course of events is extracted from news. For example, whether two events overlap or are in sequence often depends very much on their durations. If a war started yesterday, we can be pretty sure it is still going on today.", "contribution": "This paper describes one part of an exploration into how this information can be captured automatically. We have developed annotation guidelines to minimize discrepant judgments and annotated 58 articles, comprising 2288 events; we have developed a method for measuring inter-annotator agreement when the judgments are intervals on a scale; and we have shown that machine learning techniques applied to the annotated data considerably outperform a baseline and approach human performance.", "url": "https://doi.org/10.3115/1220175.1220225", "title": "Learning Event Durations From Event Descriptions"}
{"distilbart": "Interconnectivity (interdependence) between networks affects the sizes of their cascades of load shedding. For networks derived from interdependent power grids, we show that interdependence can have a stable equilibrium. An isolated network suppresses its large cascades by connecting to other networks, but too many interconnections exacerbate its largest cascades-and those of the whole system. We develop techniques to estimate this optimal amount of interconnectivity, and we examine how differences among networks' capacity and load affect this equilibrium. Our framework advances the current mathematical tools for analyzing dynamics on interdependent (or modular) networks, and it improves our understanding of systemic risk in coupled networks. In the basic process we consider, a system contains many elements that shed load to neighboring elements whenever they reach their capacity. This is captured by the classic sandpile model of Bak-Tang--Wiesenfeld, a paradigm for the power law statistics of cascades in many disciplines, from neuronal avalanches", "gold": "Understanding how interdependence among systems affects cascading behaviors is increasingly important across many fields of science and engineering.Inspired by cascades of load shedding in coupled electric grids and other infrastructure, we study the Bak-Tang-Wiesenfeld sandpile model on modular random graphs and on graphs based on actual, interdependent power grids. Starting from two isolated networks, adding some connectivity between them is beneficial, for it suppresses the largest cascades in each system. Too much interconnectivity, however, becomes detrimental for two reasons. First, interconnections open pathways for neighboring networks to inflict large cascades. Second, as in real infrastructure, new interconnections increase capacity and total possible load, which fuels even larger cascades. Using a multitype branching process and simulations we show these effects and estimate the optimal level of interconnectivity that balances their tradeoffs. Such equilibria could allow, for example, power grid owners to minimize the largest cascades in their grid. We also show that asymmetric capacity among interdependent networks affects the optimal connectivity that each prefers and may lead to an arms race for greater capacity. Our multitype branching process framework provides building blocks for better prediction of cascading processes on modular random graphs and on multi-type networks in general.", "context": "Here we analyze how the interconnectivity (interdependence) between networks affects the sizes of their cascades of load shedding. For networks derived from interdependent power grids, we show that interdependence can have a stable equilibrium. An isolated network suppresses its large cascades by connecting to other networks, but too many interconnections exacerbate its largest cascades--and those of the whole system. We develop techniques to estimate this optimal amount of interconnection, and we examine how differences among networks' capacity and load affect this equilibrium. Our framework advances the current mathematical tools for analyzing dynamics on interdependent (or modular) networks, and it improves our understanding of systemic risk in coupled networks.", "contribution": "Here we analyze how the interconnectivity (interdependence) between networks affects the sizes of their cascades of load shedding. For networks derived from interdependent power grids, we show that interdependence can have a stable equilibrium. An isolated network suppresses its large cascades by connecting to other networks, but too many interconnections exacerbate its largest cascades--and those of the whole system. We develop techniques to estimate this optimal amount of interconnection, and we examine how differences among networks' capacity and load affect this equilibrium. Our framework advances the current mathematical tools for analyzing dynamics on interdependent (or modular) networks, and it improves our understanding of systemic risk in coupled networks.", "url": "https://doi.org/10.1073/pnas.1110586109", "title": "Suppressing cascades of load in interdependent networks"}
{"distilbart": "This paper presents a solution to the above problem by developing an efficient top-k algorithm for social search and ranking. The method is based on principles of top-K threshold algorithms over inverted lists of different types, with various novel techniques for the specific setting of large online communities. For leveraging the \"social wisdom\", the algorithm employs a new form of two-dimensional expansions: social expansion considers the strength of relations among users, and semantic expansion considered the relatedness of different tags. For efficiency, these expansions are performed incrementally on demand, by dynamically folding friends and related tags into the search space. The excellent performance of the method is demonstrated by an experimental evaluation on three real-world datasets, crawled from deli.cio.us, Flickr, LibraryThing, LinkedIn, MySpace, Facebook, and YouTube.", "gold": "Online communities have become popular for publishing and searching content, as well as for finding and connecting to other users. User-generated content includes, for example, personal blogs, bookmarks, and digital photos. These items can be annotated and rated by different users, and these social tags and derived user-specific scores can be leveraged for searching relevant content and discovering subjectively interesting items. Moreover, the relationships among users can also be taken into consideration for ranking search results, the intuition being that you trust the recommendations of your close friends more than those of your casual acquaintances. Queries for tag or keyword combinations that compute and rank the top-k results thus face a large variety of options that complicate the query processing and pose efficiency challenges. This paper addresses these issues by developing an incremental top-k algorithm with two-dimensional expansions: social expansion considers the strength of relations among users, and semantic expansion considers the relatedness of different tags. It presents a new algorithm, based on principles of threshold algorithms, by folding friends and related tags into the search space in an incremental on-demand manner. The excellent performance of the method is demonstrated by an experimental evaluation on three real-world datasets, crawled from deli.cio.us, Flickr, and LibraryThing.", "context": "Social tagging offers an opportunity to exploit the \"wisdom of the crowds\" by identifying valuable content that is recommended by friends - either directly or indirectly (i.e., via friends of friends) and explicitly (e.g., ratings) or implicitly (e., by intensive tagging). This situation resembles the paradigm of collaborative recommendation, which applies data mining on customer-product and similar usage data to predict items that users are likely interested in. However, the fast growth of communities and the very high rate of content production and tagging efforts calls for highly efficient and scalable methods. Existing algorithms for fast Web search do not consider user relationships and the assets from social tagging, and prior methods for collaborative recommendation do not provide the throughput scalability that one needs for running millions of daily ad-hoc queries in social communities such as Flickr -- with more than 2 billion content items, many million users, and high dynamics.This paper presents a solution to the above problem by developing an efficient top-", "contribution": "This paper presents an efficient top-k algorithm for social search and ranking. The method is based on principles of top-$k$ threshold algorithms over inverted lists of different types, with various novel techniques for the specific setting of large online communities. For efficiency, these expansions are performed incrementally on demand, by dynamically folding friends and related tags into the search space. The excellent performance of the method is demonstrated by an experimental evaluation on three real-world datasets, crawled from deli.cio.us, Flickr, LibraryThing, LinkedIn, MySpace, Facebook, and YouTube.", "url": "https://doi.org/10.1145/1390334.1390424", "title": "Efficient top-k querying over social-tagging networks"}
{"distilbart": "Latent Dirichlet Allocation (LDA) has become the dominant unsupervised approach for aspect extraction. However, LDA models do not directly encode word co-occurrence statistics which are the primary source of information to preserve topic coherence over word types. In this work, we present a novel neural approach to tackle the weaknesses of LDA-based methods. We propose Attention-based Aspect Extraction (ABAE). Our proposed method uses dimension reduction to extract the most important aspects in the review corpus and uses an attention mechanism to remove irrelevant words to further improve coherence of the aspects. The results show that ABAE is effective in discovering meaningful and coherent aspects. It substantially outperforms baseline methods on multiple evaluation tasks. In addition, ABAEs is intuitive and structurally simple. It can also easily scale to a large amount of training data.", "gold": "AbstractAspect extraction is an important and challenging task in aspect-based sentiment analysis. Existing works tend to apply variants of topic models on this task. While fairly successful, these methods usually do not produce highly coherent aspects. In this paper, we present a novel neural approach with the aim of discovering coherent aspects. The model improves coherence by exploiting the distribution of word co-occurrences through the use of neural word embeddings. Unlike topic models which typically assume independently generated words, word embedding models encourage words that appear in similar contexts to be located close to each other in the embedding space. In addition, we use an attention mechanism to de-emphasize irrelevant words during training, further improving the coherence of aspects. Experimental results on real-life datasets demonstrate that our approach discovers more meaningful and coherent aspects, and substantially outperforms baseline methods on several evaluation tasks.", "context": "AbstractLatent Dirichlet Allocation (LDA) and its variants have become the dominant unsupervised approach for aspect extraction. LDA models the corpus as a mixture of topics (aspects), and topics as distributions over word types. While the mixture of aspects discovered by LDA-based models may describe a corpus fairly well, we find that the individual aspects inferred are of poor quality - they often consist of unrelated or loosely related concepts. This may substantially reduce users' confidence in using such automated systems. In this work, we present a novel neural approach to tackle these weaknesses. We start with neural word embeddings that already map words that usually co-occur within the same context to nearby points in the embedding space (Mikolov et al., 2013). The attention mechanism deemphasizes words that are not part of any aspect, allowing the model to focus on aspect words. Our proposed method explicitly encodes word-occurrence statistics into word embed", "contribution": "In this work, we present a novel neural approach to tackle the weaknesses of LDA-based methods. We start with neural word embeddings that already map words that usually co-occur within the same context to nearby points in the embedding space (Mikolov et al., 2013). We then filter the word embeddeddings within a sentence using an attention mechanism and use the filtered words to construct aspect embedding. The training process is analogous to autoencoders, where we use dimension reduction to extract common factors among embedded sentences and reconstruct each sentence through a linear combination of aspect embedDings. The attention mechanism deemphasizes words that are not part of any aspect, allowing the model to focus on aspect words. We call our proposed model Attention-based Aspect Extraction (ABAE). In contrast to traditional Latent Dirichlet Allocation (LDA)-based models, our method explicitly encodes word-occurrence statistics into word", "url": "https://doi.org/10.18653/v1/P17-1036", "title": "An Unsupervised Neural Attention Model for Aspect Extraction"}
{"distilbart": "This paper is to provide a discussion and overview-of some of the lessons for statistics which emerged from this contest and its data set. This vantage will also allow us to search for alternative approaches for analyzing such data (while noting some open problems), as well as to attempt to understand the commonalities and interplay among the various methods that key contestants have proposed. Netflix Inc. of Los Gatos, California, on October 2, 2006, publicly released a remarkable set of data, and offered a Grand Prize of one million US dollars to the person or team who could succeed in modeling this data to within a certain precisely defined predictive specification. While this contest attracted attention from many quarters-and most notably from within the computer science and artificial intelligence communitiesthe heart of this contest was a problem of statistical modeling, in a context known as collaborative filtering. Our goal in this paper aims at providing an overview--from a primarily statistical viewpoint--of the lessons learned for statistics--which emerged from", "gold": "Inspired by the legacy of the Netflix contest, we provide an overview of what has been learned---from our own efforts, and those of others---concerning the problems of collaborative filtering and recommender systems. The data set consists of about 100 million movie ratings (from 1 to 5 stars) involving some 480 thousand users and some 18 thousand movies; the associated ratings matrix is about 99% sparse. The goal is to predict ratings that users will give to movies; systems which can do this accurately have significant commercial applications, particularly on the world wide web. We discuss, in some detail, approaches to\"baseline\"modeling, singular value decomposition (SVD), as well as kNN (nearest neighbor) and neural network models; temporal effects, cross-validation issues, ensemble methods and other considerations are discussed as well. We compare existing models in a search for new models, and also discuss the mission-critical issues of penalization and parameter shrinkage which arise when the dimensions of a parameter space reaches into the millions. Although much work on such problems has been carried out by the computer science and machine learning communities, our goal here is to address a statistical audience, and to provide a primarily statistical treatment of the lessons that have been learned from this remarkable set of data.", "context": "In what turned out to be an invaluable contribution to the research community, Netflix Inc. of Los Gatos, California, on October 2, 2006, publicly released a remarkable set of data, and offered a Grand Prize of one million US dollars to the person or team who could succeed in modeling this data to within a certain precisely defined predictive specification. While this contest attracted attention from many quarters\u2014and most notably from within the computer science and artificial intelligence communitiesthe heart of this contest was a problem of statistical modeling, in a context known as collaborative filtering. Our goal in this paper is to provide a discussion and overview\u2014from a primarily statistical viewpoint\u2014of some of the lessons for statistics which emerged from this contest and its data set. This vantage will also allow us to search for alternative approaches for analyzing such data (while noting some open problems), as well as to understand the commonalities and interplay among the various methods that key contestants have proposed.", "contribution": "In this paper, we provide a discussion and overview of some of the lessons that emerged from this contest and its data set. This vantage will also allow us to search for alternative approaches for analyzing such data (while noting some open problems), as well as to attempt to understand the commonalities and interplay among the various methods that key contestants have proposed.", "url": "https://doi.org/10.1214/11-STS368", "title": "Statistical Significance of the Netflix Challenge"}
{"distilbart": "The Non-Negative Matrix Factorization (NMF) has been implemented as a single standalone application for Microsoft Windows platform. The application is written in Borland Delphi version 7 and it does not require any special installation or libraries and thus bioNFF can be executed in three steps: 1) Selection of the data set for analysis: bioNMF accepts as input data files tab separated text files, which might or might not contain row and column labels. In addition and for compatibility reasons, it also accepts data in the format used by the Engene software package, which allows multiple annotations per rows and columns. There is no limit on the size of the input data (number of rows and column). Only available memory might practically limits its size.2) Transformation of the dataset for normalization and to accommodate it for positive constraints: Seven normalization methods have been included to pre-process the data before the analysis. After normalization, can be accommodated into three main modules of", "gold": "In the Bioinformatics field, a great deal of interest has been given to Non-negative matrix factorization technique (NMF), due to its capability of providing new insights and relevant information about the complex latent relationships in experimental data sets. This method, and some of its variants, has been successfully applied to gene expression, sequence analysis, functional characterization of genes and text mining. Even if the interest on this technique by the bioinformatics community has been increased during the last few years, there are not many available simple standalone tools to specifically perform these types of data analysis in an integrated environment. In this work we propose a versatile and user-friendly tool that implements the NMF methodology in different analysis contexts to support some of the most important reported applications of this new methodology. This includes clustering and biclustering gene expression data, protein sequence analysis, text mining of biomedical literature and sample classification using gene expression. The tool, which is named bioNMF, also contains a user-friendly graphical interface to explore results in an interactive manner and facilitate in this way the exploratory data analysis process. bioNMF is a standalone versatile application which does not require any special installation or libraries. It can be used for most of the multiple applications proposed in the bioinformatics field or to support new research using this method. This tool is publicly available at http://www.dacya.ucm.es/apascual/bioNMF .", "context": "BACKGROUND Non-Negative Matrix Factorization (NMF) is a matrix factorization algorithm originally introduced in 1999 by Lee and Seung to decompose images into recognizable features. The main difference between NMF and other classical factorization methods relies on the non-negativity constraints imposed to the model because they allow only additive, not subtractive, combinations of data items. In this way, the factors produced by this method can be interpreted as parts of the data or, in other words, as subsets of elements that tend to occur together in sub-portions of the dataset. Therefore the interpretation of the factors becomes nonintuitive and difficult. RESULTS bioNMF has been implemented as a single standalone application for Microsoft Windows platform and it does not require any special installation or libraries. Analysis using bionmF can be executed in three steps: 1) Selection of the input data files tab separated text files, which might or might not contain row and column labels; 2) Transformation", "contribution": "RESULTS We have developed bioNMF, a user-friendly tool that implements the classical NMF algorithm and a new sparse variant, the Non-Smooth Non-Negative Matrix Factorization tailored for different applications proposed in the bioinformatics field. BioNMF has been implemented as a single standalone application for Microsoft Windows platform and it does not require any special installation or libraries and thus is self-contained in a single single application file. CONCLUSIONS The analysis step has been divided into three main modules: a) Standard NMF; b) Biclustering Analysis; c) Sample classification. All of them make use of the non-negative matrix factorization model described below.", "url": "https://doi.org/10.1186/1471-2105-7-366", "title": "bioNMF: a versatile tool for non-negative matrix factorization in biology"}
{"distilbart": "In serial block-face SEM (SBEM) and focused ion beam SEM (FIB-SEM), sections are cut by hand using an ultramicrotome. After each cut, the blockface is imaged with scanning beam before the next section is shaved off. SBEM achieves field of views of >0.5 \u00d7 0.5 mm2 at a lateral resolution on the order of 6\u201310 nm and reliable cuts thousands of sections at section thickness 20\u201330 nm for neural tissue at acquisition rate of 0.4 megapixel s\u22121 (Knott et al., 2008; Boergens & Denk, 2013). However, the major advantage of these methods is that they do not suffer from warping problems and section loss that can significantly affect the data quality and the subsequent data analysis. In contrast, the advantages of the nondestructive manual (ssTEM) ultrathin sectioning and slice collection with subsequent imaging. The latter two use", "gold": "Serial block-face scanning electron microscopy (SBEM) is becoming increasingly popular for a wide range of applications in many disciplines from biology to material sciences. This review focuses on applications for circuit reconstruction in neuroscience, which is one of the major driving forces advancing SBEM. Neuronal circuit reconstruction poses exceptional challenges to volume EM in terms of resolution, field of view, acquisition time and sample preparation. Mapping the connections between neurons in the brain is crucial for understanding information flow and information processing in the brain. However, information on the connectivity between hundreds or even thousands of neurons densely packed in neuronal microcircuits is still largely missing. Volume EM techniques such as serial section TEM, automated tape-collecting ultramicrotome, focused ion-beam scanning electron microscopy and SBEM (microtome serial block-face scanning electron microscopy) are the techniques that provide sufficient resolution to resolve ultrastructural details such as synapses and provides sufficient field of view for dense reconstruction of neuronal circuits. While volume EM techniques are advancing, they are generating large data sets on the terabyte scale that require new image processing workflows and analysis tools. In this review, we present the recent advances in SBEM for circuit reconstruction in neuroscience and an overview of existing image processing and analysis pipelines.", "context": "Dense neural circuit reconstruction requires 3D ultrastructural resolution on the nanometre scale (<20 nm pixel-1). The commonly used techniques for dense circuit reconstructions are volume EM techniques such as serial section TEM (ssTEM), automated tapecollecting ultramicrotome SEM (ATUM-SEM), serial block-face SEM (SBEM) and focused ion beam SEM (FIB-SEEM). In ssTEM, sections are cut by hand using an ultra-microtome and collected onto grids. Sections are imaged in a TEM, and imaging approaches such as camera array imaging (TEMCA) with typically 2\u20134 nm lateral resolution can acquire images at a rate of 5\u20138 megapixels s\u22121. However, the manual sectioning process is prone to errors and is also labour intensive. It limits the typical section thickness to 40\u201350 nm and the number of consecutive sections to a few thousands,", "contribution": "Here, we report the first complete connectome reconstruction of a mouse Caenorhabditis elegans neural circuit using serial section TEM (ssTEM), automated tape-collecting ultramicrotome SEM (ATUM-SEM) and serial block-face SEM (SBEM). In contrast to ssTEM, SBEM is able to reconstruct all 302 neurons connected by more than 7000 synapses at an acquisition rate of 0.5 to 2 megapixel s(-1). This allows for reliable cut thousands of subsequent sections as thin as 30 nm with a lateral resolution < 5 nm and an acquisition rates of 1.1\u20130.5 microm(2)/s(-1) and 0.3-0.4 micromol/m(2). We also show that this method can be used in situ on silicon wafers or on microfabricated silicon wafer slices. Our results demonstrate the potential use of volume EM techniques such as", "url": "https://doi.org/10.1111/jmi.12244", "title": "Challenges of microtome\u2010based serial block\u2010face scanning electron microscopy in neuroscience"}
{"distilbart": "This article reviews 10 years of research on ISIS, a system that provides tools to support the construction of reliable distributed software. The thesis underlying ISIS is that development of reliable software can be simplified using process groups and group programming tools. This article describes the approach taken, surveys the system, and discusses experiences with real applications. It will be helpful to illustrate group programming and ISIS in a setting where the system has found rapid acceptance: brokerage and trading systems. These systems integrate large numbers of demanding applications and require timely reaction to high volumes of pricing and trading information. Trading strategies rely on accurate pricing and market-volatility data, dynamically changing databases giving the firm's holdings in various equities, news and analysis data, and elaborate financial and economic models based on relationships between financial instruments. Any distributed system in support of this application must serve multiple communities: the firm as a whole, where reliability and security are key considerations; the brokers who depend on speed and the ability to customize the trading environment;", "gold": "One might expect the reliability of a distributed system to correspond directly to the reliability of its constituents, but this is not always the case. The mechanisms used to structure a distributed system and to implement cooperation between components play a vital role in determining", "context": "This article reviews 10 years of research on ISIS, a system that provides tools to support the construction of reliable distributed software. The thesis underlying ISIS is that development of reliability can be simplified using process groups and group programming tools. This article describes the approach taken, surveys the system, and discusses experiences with real applications. It will be helpful to illustrate group programming and ISIS in a setting where the system has found rapid acceptance: brokerage and trading systems. These systems integrate large numbers of demanding applications and require timely reaction to high volumes of pricing and trading information.", "contribution": "This article reviews 10 years of research on ISIS, a system that provides tools to support the construction of reliable distributed software. This article describes the approach taken, surveys the system, and discusses experiences with real applications. It will be helpful to illustrate group programming and ISIS in a setting where the system has found rapid acceptance: brokerage and trading systems.", "url": "https://doi.org/10.1145/163298.163303", "title": "The process group approach to reliable distributed computing"}
{"distilbart": "We propose an unsupervised learning framework for performing cross-city semantic segmentation. Our proposed model is able to adapt a pre-trained segmentation model to a new city of interest, while only the collection of unlabeled road scene images of that city is required. To avoid any human interaction or annotation during data collection, we utilize Google Street View with its time-machine 1 feature to harvest road scenes images taken at the same locations but across different times. As detailed later in Sec. 4, this allows us to extract static-object priors from the collected image data, without the need of user annotations or interaction. By integrating such priors with the proposed global and class-specific domain adversarial learning framework, refining/adapting the pre-training segmenter can be easily realized. The main contributions of this paper can be summarized as follows:\u2022 We propose an approach which performs global andclass-wise adaptation for deploying pre- trained road scene segmenters across cities.", "gold": "Despite the recent success of deep-learning based semantic segmentation, deploying a pre-trained road scene segmenter to a city whose images are not presented in the training set would not achieve satisfactory performance due to dataset biases. Instead of collecting a large number of annotated images of each city of interest to train or refine the segmenter, we propose an unsupervised learning approach to adapt road scene segmenters across different cities. By utilizing Google Street View and its time-machine feature, we can collect unannotated images for each road scene at different times, so that the associated static-object priors can be extracted accordingly. By advancing a joint global and class-specific domain adversarial learning framework, adaptation of pre-trained segmenters to that city can be achieved without the need of any user annotation or interaction. We show that our method improves the performance of semantic segmentation in multiple cities across continents, while it performs favorably against state-of-the-art approaches requiring annotated training data.", "context": "Recent developments of technologies in computer vision, deep learning, and more broadly artificial intelligence have led to the race of building advanced driver assistance systems (ADAS). From recognizing particular objects of interest toward understanding the corresponding driving environments, road scene segmentation is among the key components for a successful ADAS. With a sufficient amount of annotated training image data, existing computer vision algorithms already exhibit promising performances on the above task. However, when one applies pre-trained seg* indicates equal contribution, the resulting performance would be degraded due to dataset (domain) biases. We conduct a pilot experiment to illustrate how severe a state-of-the-art semantic segmenter would be affected by the above dataset bias problem. We consider the segmenter of [2] which is trained on Cityscapes, and apply for segmenting about 400 annotated road scene images of different cities across countries: Rome, Rio, Taipei, and Tokyo. A drop in mean of intersection over union (m", "contribution": "Inspired by the recent advances in domain adaptation, we propose an unsupervised learning framework for performing cross-city semantic segmentation. Our proposed model is able to adapt a pre-trained segmentation model to a new city of interest while only the collection of unlabeled road scene images of that city is required. To avoid any human interaction or annotation during data collection, we utilize Google Street View with its time-machine 1 feature to harvest road scenes taken at the same (or nearby) locations but across different times. As detailed later in Sec. 4, this allows us to extract static-object priors from the collected image data, without the need of user annotation or interaction. By integrating such priors with the proposed global and class-specific domain adversarial learning framework, refining/adapting the pre- trained segmenter can be easily realized. We conduct a pilot experiment to illustrate how severe a state-of-the-art semantic segmenter would be affected by the", "url": "https://doi.org/10.1109/ICCV.2017.220", "title": "No More Discrimination: Cross City Adaptation of Road Scene Segmenters"}
{"distilbart": "BACKGROUND Epilepsy is a chronic condition of the brain, and epileptic seizures can occur in one or more unprovoked seizures. The aim of this paper was to use EEG recordings from 24 patients suffering with epilepsy. MATERIALS AND METHODOLOGY/PRINCIPAL FINDINGS In order to improve our understanding of epileptic seizure detection using EEG data, we used an electroencephalogram (EEG) classifier for classification accuracy of 96% for sensitivity and false-positive rate of 0.08 per hour. The results show that 83.6% was for specificity, 90.8% for specificity and 98.9% for false-negative rate of 1.2 \u00b1 2.0 per hour, respectively. The main difference between these approaches taken in the CHB-MIT database and the support vector machine (SVM). The SVM classifiers were trained on one person and tested on the same person without prior knowledge of which region of", "gold": "Epilepsy is a chronic neurological condition that affects approximately 70 million people worldwide. Characterised by sudden bursts of excess electricity in the brain, manifesting as seizures, epilepsy is still not well understood when compared with other neurological disorders. Seizures often happen unexpectedly and attempting to predict them has been a research topic for the last 30 years. Electroencephalograms have been integral to these studies, as the recordings that they produce can capture the brain\u2019s electrical signals. The diagnosis of epilepsy is usually made by a neurologist, but can be difficult to make in the early stages. Supporting para-clinical evidence obtained from magnetic resonance imaging and electroencephalography may enable clinicians to make a diagnosis of epilepsy and instigate treatment earlier. However, electroencephalogram capture and interpretation is time consuming and can be expensive due to the need for trained specialists to perform the interpretation. Automated detection of correlates of seizure activity generalised across different regions of the brain and across multiple subjects may be a solution. This paper explores this idea further and presents a supervised machine learning approach that classifies seizure and non-seizure records using an open dataset containing 342 records (171 seizures and 171 non-seizures). Our approach posits a new method for generalising seizure detection across different subjects without prior knowledge about the focal point of seizures. Our results show an improvement on existing studies with 88% for sensitivity, 88% for specificity and 93% for the area under the curve, with a 12% global error, using the k-NN classifier.", "context": "In this paper, a whole-brain seizure detection approach supports para-clinical evidence obtained from magnetic resonance imaging and electroencephalogram (EEG) to make a diagnosis of epilepsy and instigate treatment earlier. More importantly, it mitigates the difficulties associated with the capture and interpretation of EEG classification to generalise detection across all regions of the brain using multiple subject records, without prior knowledge of which region the seizure occurred. A robust data processing methodology is adopted and several classifiers are trained and evaluated, using 342 EEG segments (171 seizures and 171 non-seizures) extracted from the EEG records of 24 patients suffering with epilepsy. The results show that a classification accuracy of 96% for sensitivity was produced, with a false-positive rate of 0.08 per hour.", "contribution": "In this paper, a robust data processing methodology is adopted and several classifiers are trained and evaluated, using 342 EEG segments (171 seizures and 171 non-seizures) extracted from the EEG records of 24 patients suffering with epilepsy. The results show that a classification accuracy of 96% for sensitivity was produced, with a false-positive rate of 0.08 per hour.", "url": "https://doi.org/10.1016/j.aci.2015.01.001", "title": "A machine learning system for automated whole-brain seizure detection"}
{"distilbart": "The notion of unbounded concurrent two-party computation can not be achieved in general. In the case of a dishonest majority, we show that unbounded concurrency is guaranteed even under an unbounded number of concurrent executions (i.e, bounded concurrency). We note here that this technique is inherently bound to fail when many copies of the protocol are executed concurrently and that it does not necessarily imply its security in the, more demanding, concurrent setting. It has been shown that any probabilistic polynomial-time computable multi-party functionality can be securely computed, regardless of the number of corrupted parties, assuming the existence of enhanced trapdoor permutations. This should hold even in case that a subset of the parties maliciously deviates from the protocol, such that party Pi receives the value fi(x) is a random variable. Loosely speaking, the security requirements are that the parties learn nothing from their prescribed output, and that the output of each party is distributed", "gold": "We show how to securely realize any multi-party functionality in a way that preserves security under an a-priori bounded number of concurrent executions, regardless of the number of corrupted parties. Previous protocols for the above task either rely on set-up assumptions such as a Common Reference String, or require an honest majority. Our constructions are in the plain model and rely on standard intractability assumptions (enhanced trapdoor permutations and collision resistant hash functions). Even though our main focus is on feasibility of concurrent multi-party computation we actually obtain a protocol using only a constant number of communication rounds. As a consequence our protocol yields the first construction of constant-round phstand-alone secure multi-party computation with a dishonest majority, proven secure under standard (polynomial-time) hardness assumptions; previous solutions to this task either require logarithmic round-complexity, or subexponential hardness assumptions. The core of our protocol is a novel construction of (concurrently) simulation-sound zero-knowledge protocols, which might be of independent interest. Finally, we extend the framework constructed to give a protocol for secure multi-party (and thus two-party) computation for any number of corrupted parties, which remains secure even when arbitrary subsets of parties concurrently execute the protocol, possibly with interchangeable roles. As far as we know, for the case of two-party or multi-party protocols with a dishonest majority, this is the first positive result for any non-trivial functionality which achieves this property in the plain model.", "context": "Abstract. The task of secure multi-party computation involves n parties P1,..., Pn that wish to jointly and securely compute a functionality f(x) = (f1(x),..., fn(x)) of their corresponding private inputs x=x1,..., xn, such that party Pi receives the value fi(x). This functionality may be probabilistic in which case fi(X) is a random variable. Loosely speaking, the security requirements are that the parties learn nothing more from the protocol than their prescribed output, and that the output of each party is distributed according to the prescribed functionality. This should hold even in case that a subset of the parties maliciously deviates from the Protocol. Shortly after its conceptualization, very strong results were established for secure multi\u2010party computation. It has been noticed that security of a specific protocol in the stand\u2011alone setting does not necessarily imply its security in the, more demanding, concurrent setting", "contribution": "In the current work, we show that unbounded concurrent two-party computation can be achieved using only O(1) communication rounds. In particular, in the special case of zero-knowledge proofs, security is guaranteed even under an unbounded number of concurrent executions and a dishonest majority. Furthermore, our results also extend to the multi-party setting with honest majority (with or without black-box simulation).", "url": "https://doi.org/10.1145/1007352.1007393", "title": "Bounded-concurrent secure multi-party computation with a dishonest majority"}
{"distilbart": "The field of bibliometrics is concerned with conducting quantitative analyses of documents, traditionally printed ones such as books and journal articles. Bibliometric techniques have recently been applied to online journals (Harter & Ford, 2000), individual ejournals (Marek & Valauskas, 2002) and subject-based digital libraries including preprint archives (Brody et al., 2001). This may represent a long-term shift towards online analysis, even for formal scholarly communication. The most well-known publication impact metrics are the journal impact factors of the Institute for Scientific Information, although these are themselves problematic and controversial. In particular, he recommends that a range of indicators should be used. For whole universities formal evaluation is normally not required but quantitative approaches are still used at the national, university and lower organizational levels to track patterns of scholarly communication, both nationally and internationally, either using citations, cocitations or co-authorships or to design effective Web sites. Social scientists may be more", "gold": "The quality and impact of academic Web sites is of interest to many audiences, including the scholars who use them and Web educators who need to identify best practice. Several large-scale European Union research projects have been funded to build new indicators for online scientific activity, reflecting recognition of the importance of the Web for scholarly communication. In this paper we address the key question of whether higher rated scholars produce higher impact Web sites, using the UK as a case study and measuring scholars' quality in terms of university-wide average research ratings. Methodological issues concerning the measurement of the online impact are discussed, leading to the adoption of counts of links to a university's constituent single domain Web sites from an aggregated counting metric. The findings suggest that universities with higher rated scholars produce significantly more Web content but with a similar average online impact. Higher rated scholars therefore attract more total links from their peers, but only by being more prolific, refuting earlier suggestions. It can be surmised that general Web publications are very different from scholarly journal articles and conference papers, for which scholarly quality does associate with citation impact. This has important implications for the construction of new Web indicators, for example that online impact should not be used to assess the quality of small groups of scholars, even within a single discipline.", "context": "In this paper, we introduce a new metric for measuring the impact of Web spaces such as university Web sites. This measure is based on the ratio of links pointing to the space in question from outside of that space (inlinks) divided by the total number of pages in the space. We compare it with other measures of research quality mainly at the national and university levels, including citations between journal articles, conference papers and preprint archives. In addition, we present an empirical study of the relationship between these metrics. Our results show that there are significant differences in terms of citation rates among different types of Web data sets. The most important finding was that the correlation coefficients were significantly higher than those obtained using traditional bibliometric methods but lower than those used in previous studies. Furthermore, we found no evidence of a difference in citation rate when comparing the two types of metrics. However, our findings do not support the use of one type of metric in comparison to another type.", "contribution": "In this paper, we introduce a new metric that can be used to assess the impact of Web spaces such as university Web sites. This measure is based on the ratio of links pointing to the space in question from outside of that space (inlinks) divided by the total number of pages in the space. We compare it with other measures of research quality mainly at the national and university levels. Our results show that there are significant differences between the two metrics in terms of citation counts for each one of the three indicators: citations, cocitations and co-authorships. In addition, we find no evidence of an increase or decrease in citation counts over time. The most important finding was that citations were significantly higher among universities than among departments and institutes where they had been published more frequently compared to those who did not publish.", "url": "https://doi.org/10.1002/asi.10362", "title": "Do The Web Sites of Higher Rated Scholars Have Significantly More Online Impact ?"}
{"distilbart": "Self-driving vehicles are an effective tool to learn vehicle controllers for self-driving cars in an end-to-end manner. Despite their effectiveness as function estimators, DNNs are typically cryptic black boxes. There are no explainable states or labels in such a network, and representations are fully distributed as sets of activations. Explainable models that make deep models more transparent are important for a number of reasons: (i) user acceptance -self-driver vehicles are a radical technology for users to accept, and require a very high level of trust, (ii) understanding and extrapolation of vehicle behavior -users ideally should be able to anticipate what the vehicle will do in most situations, (iii) effective communication -they help user communicate preferences to the vehicle and vice versa, while an explanation model generates a natural language explanation of the rationales, e.g., \"The car is driving forward because there are no other cars in its lane\", and a visual explanation in the", "gold": "Deep neural perception and control networks have become key components of self-driving vehicles. User acceptance is likely to benefit from easy-to-interpret textual explanations which allow end-users to understand what triggered a particular behavior. Explanations may be triggered by the neural controller, namely introspective explanations, or informed by the neural controller's output, namely rationalizations. We propose a new approach to introspective explanations which consists of two parts. First, we use a visual (spatial) attention model to train a convolutional network end-to-end from images to the vehicle control commands, i.e., acceleration and change of course. The controller's attention identifies image regions that potentially influence the network's output. Second, we use an attention-based video-to-text model to produce textual explanations of model actions. The attention maps of controller and explanation model are aligned so that explanations are grounded in the parts of the scene that mattered to the controller. We explore two approaches to attention alignment, strong- and weak-alignment. Finally, we explore a version of our model that generates rationalizations, and compare with introspective explanations on the same video segments. We evaluate these models on a novel driving dataset with ground-truth human explanations, the Berkeley DeepDrive eXplanation (BDD-X) dataset. Code is available at https://github.com/JinkyuKimUCB/explainable-deep-driving.", "context": "In this work, we propose an introspective textual explanation model for self-driving cars to provide easy-to-interpret explanations for the behavior of a deep vehicle control network. We integrate our explanation generator with the controller by aligning their attentions to ground the explanation, and compare two approaches: attention-aligned explanations and non-aligned rationalizations. We generated a large-scale Berkeley DeepDrive eXplanation (BDD-X) dataset with over 6,984 video clips annotated with driving descriptions, e.g., \"The car slows down\" and \"because it is approaching an intersection and the light is red\" as in Figure 1. Our dataset provides a new testbed for measuring progress towards developing explainable models for self\u2013driving cars.", "contribution": "In this work, we propose an introspective textual explanation model for self-driving cars to provide easy-to-interpret explanations for the behavior of a deep vehicle control network. We integrate our explanation generator with the controller by aligning their attentions to ground the explanation and compare two approaches: attention-aligned explanations and non-aligned rationalizations. We generated a large-scale Berkeley DeepDrive eXplanation (BDD-X) dataset with over 6,984 video clips annotated with driving descriptions, e.g., \"The car slows down\" and \"because it is approaching an intersection and the light is red\". Our experiments show that using attention alignment between controller and explanation models generally improves the quality of explanations which better match the human rationalizations of the driving videos.", "url": "https://doi.org/10.1007/978-3-030-01216-8_35", "title": "Textual Explanations for Self-Driving Vehicles"}
